{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "        sys.path.append(path)\n",
    "add_path('/home/jjian03/anaconda3/lib/python3.7/site-packages')\n",
    "add_path(f'{os.path.abspath(os.path.join(\".\"))}/lib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.Repository import *\n",
    "from lib.Utility import *\n",
    "from lib.modeling import *\n",
    "from lib.preprocessing import *\n",
    "from lib.preprocessing.HTMLParser import html_parser\n",
    "from lib.viz import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(spark, path, name):\n",
    "    return spark.read.parquet(path).registerTempTable(name)\n",
    "\n",
    "def shape(df):\n",
    "    print((df.count(), len(df.columns)))\n",
    "\n",
    "try:\n",
    "    print(spark.version)\n",
    "except NameError as e:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder. \\\n",
    "                config('spark.app.name', 'Tobit regression'). \\\n",
    "                config('spark.dynamicAllocation.enabled', 'true'). \\\n",
    "                config('spark.dynamicAllocation.maxExecutors', '50'). \\\n",
    "                config('spark.dynamicAllocation.executorIdleTimeout', '30s'). \\\n",
    "                config('spark.driver.maxResultSize', '8g'). \\\n",
    "                config('spark.driver.memory', '50g'). \\\n",
    "                config('spark.executor.memory', '10g'). \\\n",
    "                config('spark.task.maxFailures', '3'). \\\n",
    "                config('spark.yarn.am.memory', '50g'). \\\n",
    "                config('spark.yarn.max.executor.failures', '3'). \\\n",
    "                config('spark.kryoserializer.buffer.max', '1024m'). \\\n",
    "                config('spark.yarn.executor.memoryOverhead', '50g'). \\\n",
    "                config('spark.executorEnv.PYTHON_EGG_CACHE', '/home/jjian03/cache'). \\\n",
    "                getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    spark_sql = SQLContext(sc)\n",
    "    print(spark.version)\n",
    "\n",
    "    load_dataset(spark, '/user/jjian03/WebResourceQuality.parquet', 'web_resource_quality')\n",
    "    load_dataset(spark, '/user/jjian03/WebResourceQuality_pmid.parquet', 'web_resource_quality_pmid')\n",
    "    load_dataset(spark, '/datasets/MAG_20200403/MAG_Azure_Parquet/mag_parquet/Papers.parquet', 'Paper')\n",
    "    load_dataset(spark, '/user/lliang06/icon/MAG_publication_features.parquet', 'mag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fract = 0.003\n",
    "\n",
    "raw_data = spark_sql.sql(f'''\n",
    "        SELECT wr.id\n",
    "            , wr.url\n",
    "            , wr.actual_scrape_url\n",
    "            , wr.first_appear\n",
    "            , wr.first_available_timestamp\n",
    "            , wr.last_available_timestamp\n",
    "            , wr.header\n",
    "            , wr.html_text\n",
    "            , wr.comment\n",
    "            , wr.from_waybackmachine\n",
    "            , wr.http_status_code\n",
    "            , wr.original_check_failure\n",
    "            , wr.original_check_error_log\n",
    "            , wr.terminate_reason\n",
    "            , wr.terminate_reason_error_log\n",
    "\n",
    "            , m.paperId\n",
    "            , m.total_num_of_paper_citing\n",
    "            , m.total_num_of_author_citing\n",
    "            , m.total_num_of_affiliation_citing\n",
    "            , m.total_num_of_journal_citing\n",
    "            , m.total_num_of_author_self_citation\n",
    "            , m.total_num_of_affiliation_self_citation\n",
    "            , m.total_num_of_journal_self_citation\n",
    "            , m.avg_year\n",
    "            , m.min_year\n",
    "            , m.max_year\n",
    "            , m.median\n",
    "            , m.num_of_author\n",
    "            , m.num_of_author_citing\n",
    "            , m.num_of_affiliation_citing\n",
    "            , m.num_of_journal_citing\n",
    "            , m.avg_hindex\n",
    "            , m.first_author_hindex\n",
    "            , m.last_author_hindex\n",
    "            , m.avg_mid_author_hindex\n",
    "            , m.paper_unique_affiliation\n",
    "\n",
    "        FROM web_resource_quality wr\n",
    "        JOIN web_resource_quality_pmid wr_doi ON wr.id = wr_doi.id\n",
    "        JOIN Paper p ON wr_doi.doi = p.doi\n",
    "        JOIN mag m ON p.paperId = m.paperId\n",
    "        WHERE wr.label IS NOT NULL\n",
    "        AND wr.label IN ('0', '1')\n",
    "        AND isNaN(wr.label) = false\n",
    "        AND wr.first_appear IS NOT NULL\n",
    "        AND isNaN(wr.first_appear) = false\n",
    "        AND lower(wr.url) NOT LIKE \"%doi.org%\"\n",
    "    ''') \\\n",
    "    .orderBy(fn.rand(seed=seed)) \\\n",
    "    .sample(False, fract, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "start_time = time.time()\n",
    "\n",
    "# raw_data.printSchema()\n",
    "\n",
    "print(f'raw_data: {shape(raw_data)}')\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Tobit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml.regression import Params, HasRegParam, HasElasticNetParam, HasMaxIter, Param, \\\n",
    "    TypeConverters, HasInputCol, HasRawPredictionCol, HasPredictionCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.sql.functions import rand\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class HasLogSigma(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param log sigma: log sigma names.\n",
    "    \"\"\"\n",
    "\n",
    "    logSigma = Param(Params._dummy(), \"logSigma\", \"log sigma names.\", typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasLogSigma, self).__init__()\n",
    "\n",
    "    def setLogSigma(self, value):\n",
    "        return self._set(logSigma=value)\n",
    "\n",
    "    def getLogSigma(self):\n",
    "        \"\"\"\n",
    "        Gets the value of log sigma or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.logSigma)\n",
    "\n",
    "\n",
    "class HasLearningRate(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param learningRate: learning rate of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    learningRate = Param(Params._dummy(), \"learningRate\", \"Learning Rate of the model.\", typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasLearningRate, self).__init__()\n",
    "\n",
    "    def setLearningRate(self, value):\n",
    "        return self._set(learningRate=value)\n",
    "\n",
    "    def getLearningRate(self):\n",
    "        \"\"\"\n",
    "        Gets the value of learningRate or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.learningRate)\n",
    "\n",
    "\n",
    "class HasCoefficients(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param coefficients: coefficients of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    coefficients = Param(Params._dummy(), \"coefficients\", \"Coefficients of the model.\", typeConverter=TypeConverters.toList)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasCoefficients, self).__init__()\n",
    "\n",
    "    def setCoefficients(self, value):\n",
    "        return self._set(coefficients=value)\n",
    "\n",
    "    def getCoefficients(self):\n",
    "        \"\"\"\n",
    "        Gets the value of coefficients or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.coefficients)\n",
    "\n",
    "\n",
    "class _LassoTobitParameter(HasCoefficients, HasLogSigma, HasRegParam, HasElasticNetParam, HasMaxIter):\n",
    "\n",
    "    leftCensorPoint = Param(Params._dummy(), \"leftCensorPoint\",\n",
    "                   \"Censored threshold on the left hand side\",\n",
    "                   typeConverter=TypeConverters.toFloat)\n",
    "    rightCensorPoint = Param(Params._dummy(), \"rightCensorPoint\",\n",
    "                   \"Censored threshold on the right hand side\",\n",
    "                   typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def setLeftCensorPoint(self, value):\n",
    "        return self._set(leftCensorPoint=value)\n",
    "\n",
    "    def getLeftCensorPoint(self):\n",
    "        \"\"\"\n",
    "        Gets the value of :py:attr:`leftCensorPoint` or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.leftCensorPoint)\n",
    "\n",
    "    def setRightCensorPoint(self, value):\n",
    "        return self._set(rightCensorPoint=value)\n",
    "\n",
    "    def getRightCensorPoint(self):\n",
    "        \"\"\"\n",
    "        Gets the value of :py:attr:`rightCensorPoint` or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.rightCensorPoint)\n",
    "\n",
    "\n",
    "def _parallelFitTasks(est, train, eva, validation, epm):\n",
    "    \"\"\"\n",
    "    Creates a list of callables which can be called from different threads to fit and evaluate\n",
    "    an estimator in parallel. Each callable returns an `(index, metric)` pair.\n",
    "\n",
    "    :param est: Estimator, the estimator to be fit.\n",
    "    :param train: DataFrame, training data set, used for fitting.\n",
    "    :param eva: Evaluator, used to compute `metric`\n",
    "    :param validation: DataFrame, validation data set, used for evaluation.\n",
    "    :param epm: Sequence of ParamMap, params maps to be used during fitting & evaluation.\n",
    "    :return: (int, float), an index into `epm` and the associated metric value.\n",
    "    \"\"\"\n",
    "    modelIter = est.fitMultiple(train, epm)\n",
    "\n",
    "    def singleTask():\n",
    "        index, model = next(modelIter)\n",
    "        eva_copy = copy.copy(eva)\n",
    "        eva_copy.model = model.coefficients\n",
    "        metric = eva.evaluate(model.transform(validation, epm[index]))\n",
    "        return index, metric\n",
    "\n",
    "    return [singleTask] * len(epm)\n",
    "\n",
    "\n",
    "\n",
    "class TobitCrossValidator(CrossValidator):\n",
    "    \"\"\"\n",
    "    Avoid mulitple calculation on coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3,\n",
    "                 seed=None, parallelism=1, collectSubModels=False):\n",
    "        \"\"\"\n",
    "        __init__(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3,\\\n",
    "                 seed=None, parallelism=1, collectSubModels=False)\n",
    "        \"\"\"\n",
    "        super(TobitCrossValidator, self).__init__()\n",
    "        self._setDefault(numFolds=3, parallelism=1)\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        nFolds = self.getOrDefault(self.numFolds)\n",
    "        seed = self.getOrDefault(self.seed)\n",
    "        h = 1.0 / nFolds\n",
    "        randCol = self.uid + \"_rand\"\n",
    "        df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
    "        metrics = [0.0] * numModels\n",
    "\n",
    "        pool = ThreadPool(processes=min(self.getParallelism(), numModels))\n",
    "\n",
    "        for i in range(nFolds):\n",
    "            validateLB = i * h\n",
    "            validateUB = (i + 1) * h\n",
    "            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n",
    "            validation = df.filter(condition).cache()\n",
    "            train = df.filter(~condition).cache()\n",
    "\n",
    "            tasks = _parallelFitTasks(est, train, eva, validation, epm)\n",
    "            for j, metric in pool.imap_unordered(lambda f: f(), tasks):\n",
    "                metrics[j] += (metric / nFolds)\n",
    "            validation.unpersist()\n",
    "            train.unpersist()\n",
    "\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "        bestModel = est.fit(dataset, epm[bestIndex])\n",
    "        return self._copyValues(CrossValidatorModel(bestModel, metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoTobitRegression(Estimator, HasInputCol, HasRawPredictionCol, HasPredictionCol,\n",
    "                           HasLearningRate, _LassoTobitParameter,\n",
    "                           DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, rawPredictionCol=None, predictionCol=None,\n",
    "                 coefficients: list=None, logSigma: float=None,\n",
    "                 regParam: float=None, elasticNetParam: float=None, maxIter: int=None,\n",
    "                 leftCensorPoint: float=None, rightCensorPoint: float=None,\n",
    "                 learningRate: float = None,\n",
    "                 ):\n",
    "        super(LassoTobitRegression, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, rawPredictionCol=None, predictionCol=None,\n",
    "                  coefficients: list=None, logSigma: float=None,\n",
    "                  regParam: float = None, elasticNetParam: float = None, maxIter: int = None,\n",
    "                  leftCensorPoint: float = None, rightCensorPoint: float = None,\n",
    "                  learningRate: float = None,\n",
    "                  ):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        X = self.getInputCol()\n",
    "        y = self.getRawPredictionCol()\n",
    "        lbd = self.getRegParam()\n",
    "        alpha = self.getElasticNetParam()\n",
    "        maxIter = self.getMaxIter()\n",
    "        left = self.getLeftCensorPoint()\n",
    "        right = self.getRightCensorPoint()\n",
    "        learningRate = self.getLearningRate()\n",
    "\n",
    "        # initialize with OLS\n",
    "        coef = self.getCoefficients()\n",
    "        if len(coef) != len(dataset.schema.names):\n",
    "            print('Column does not match!')\n",
    "            print(*coef, sep='\\n')\n",
    "            print('------')\n",
    "            print(*dataset.schema.names, sep='\\n')\n",
    "            raise AssertionError('Column does not match!')\n",
    "        logSigma = self.getLogSigma()\n",
    "\n",
    "        gradient = \n",
    "        print('шонч╗Г')\n",
    "        coefficients = None\n",
    "        \n",
    "        return LassoTobitRegressionModel(\n",
    "            inputCol=c, predictionCol=self.getPredictionCol(),\n",
    "            regParam=self.getRegParam(),\n",
    "            elasticNetParam=self.getElasticNetParam(),\n",
    "            maxIter=self.getMaxIter(),\n",
    "            leftCensorPoint=self.getLeftCensorPoint(),\n",
    "            rightCensorPoint=self.getRightCensorPoint(),\n",
    "            coefficients=coefficients,\n",
    "            logSigma=self.getLogSigma(),\n",
    "        )\n",
    "\n",
    "\n",
    "class LassoTobitRegressionModel(Model,\n",
    "                                HasInputCol, HasRawPredictionCol, HasPredictionCol,\n",
    "                                _LassoTobitParameter,\n",
    "                                DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, predictionCol=None,\n",
    "                 coefficients: list=None, logSigma: float=None,\n",
    "                 regParam=None, elasticNetParam=None, maxIter=None,\n",
    "                 leftCensorPoint=None, rightCensorPoint=None,\n",
    "                 ):\n",
    "        super(LassoTobitRegressionModel, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, predictionCol=None,\n",
    "                  regParam=None, elasticNetParam=None, maxIter=None,\n",
    "                  leftCensorPoint=None, rightCensorPoint=None,\n",
    "                  coefficients=None,\n",
    "                  ):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        x = self.getInputCol()\n",
    "        y = self.getPredictionCol()\n",
    "        coefficients = self.getCoefficients()\n",
    "\n",
    "        # Predict\n",
    "        # return dataset.withColumn(y, (dataset[x] - mu) > threshold * sigma)\n",
    "        return dataset.withColumn('Clever', fn.col('x'))\n",
    "\n",
    "\n",
    "# Test\n",
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "class LassoTobitEvaluator(Evaluator, HasInputCol, HasRawPredictionCol, HasPredictionCol):\n",
    "\n",
    "    def __init__(self, inputCol='features', predictionCol=\"prediction\", rawPredictionCol=\"rawPredictionCol\",\n",
    "                 model: Model=None,\n",
    "                 ):\n",
    "        self.inputCol = inputCol\n",
    "        self.predictionCol = predictionCol\n",
    "        self.rawPredictionCol = rawPredictionCol\n",
    "        self.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def _censored_udf(x):\n",
    "        pass\n",
    "\n",
    "    def _calculate_loglik():\n",
    "        pass\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def _evaluate(self, dataset):\n",
    "        \"\"\"\n",
    "        Returns a random number.\n",
    "        Implement here the true metric\n",
    "        \"\"\"\n",
    "        # calculate loglik\n",
    "        ll = self.model.getLeftCensorPoint()\n",
    "        rl = self.model.getRightCensorPoint()\n",
    "        logSigma = self.model.getLogSigma()\n",
    "        label = dataset.select(udf(LassoTobitEvaluator._censored_udf(self.getPredictionCol()), IntegerType()))\n",
    "        X = dataset.withColumn('intercept', fn.lit(1)) \\\n",
    "            .select([fn.col(col_name) for col_name in ['intercept', *self.getInputCol()]])\n",
    "        y = dataset.select(fn.col(self.getRawPredictionCol()))\n",
    "        coef = model.getCoefficients()\n",
    "        \n",
    "        xb = X@coef\n",
    "        \n",
    "        X = X.select(fn.col(col_names[len(col_names)-1]))\n",
    "        col_names = dataset.schema.names\n",
    "        label_udf = udf(lambda x: ,FloatType())\n",
    "          uncensored <- sum(log(dnorm(((Y[which(I>ll & I <ul)] - xb[which(I>ll & I <ul)])/ sigma), mean = 0, sd = 1)) - log(sigma))\n",
    "\n",
    "          # The alive resources are considered to be censored. They are labeled as 1\n",
    "          # Only right censored term applied for our case\n",
    "          ll_censored <- sum(log(1-pnorm((xb[which(I<=ll)]) / sigma, mean = 0, sd = 1)))\n",
    "          ul_censored <- sum(log(1-pnorm((ul - xb[which(I>=ul)]) / sigma, mean = 0, sd = 1)))\n",
    "        return ll_censored + uncensored + ul_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "df = sc.parallelize([(1, 2.0), (2, 3.0), (3, 0.0), (4, 99.0)]).toDF([\"id\", \"x\"])\n",
    "\n",
    "lasso_tobit_regressor = LassoTobitRegression() \\\n",
    "    .setInputCol(\"x\") \\\n",
    "    .setLeftCensorPoint(0) \\\n",
    "    .setRightCensorPoint(100) \\\n",
    "    .setRegParam(1) \\\n",
    "    .setMaxIter(100) \\\n",
    "    .setElasticNetParam(1)\n",
    "pipe  = Pipeline(stages=[lasso_tobit_regressor])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lasso_tobit_regressor.leftCensorPoint, [0]) \\\n",
    "    .addGrid(lasso_tobit_regressor.rightCensorPoint, [100]) \\\n",
    "    .addGrid(lasso_tobit_regressor.regParam, [1]) \\\n",
    "    .addGrid(lasso_tobit_regressor.maxIter, [100]) \\\n",
    "    .addGrid(lasso_tobit_regressor.elasticNetParam, [1]) \\\n",
    "    .build()\n",
    "\n",
    "# evaluator = LassoTobitEvaluator(labelCol='price')\n",
    "evaluator = LassoTobitEvaluator()\n",
    "\n",
    "# import inspect\n",
    "\n",
    "# print(inspect.getsource(CrossValidator()._fit))\n",
    "crossval = TobitCrossValidator() \\\n",
    "    .setEstimator(pipe) \\\n",
    "    .setEstimatorParamMaps(paramGrid) \\\n",
    "    .setEvaluator(evaluator) \\\n",
    "    .setNumFolds(2)\n",
    "# crossval = TobitCrossValidator(estimator=pipe,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=2)\n",
    "cvModel = crossval.fit(df)\n",
    "bestModel = cvModel.bestModel\n",
    "preds = bestModel.transform(df)\n",
    "\n",
    "preds.show()\n",
    "\n",
    "# cvModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('/home/jjian03/lib/pymongo-3.10.1-py2.7-macosx-10.14-intel.egg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as t\n",
    "from bson import ObjectId\n",
    "import bson\n",
    "\n",
    "\n",
    "df = sc.parallelize([('5ecd87e7150a1889d703ea37', 2.0), ('5ecd87e7150a1889d703ea37', 3.0)]).toDF([\"id\", \"x\"])\n",
    "def _extract_year_udf(oid_str):\n",
    "    def _get_year_from_id(oid_str):\n",
    "        return type(oid_str)\n",
    "#         return ObjectId(oid_str).generation_time.year\n",
    "    return ObjectId(oid_str).generation_time.year\n",
    "\n",
    "    return oid_str\n",
    "df.withColumn('y', fn.udf(_extract_year_udf, t.StringType())('id')).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import types as t\n",
    "from bson import ObjectId\n",
    "\n",
    "\n",
    "def _extract_year_udf(oid_str):\n",
    "    def _get_year_from_id(oid_str):\n",
    "        return ObjectId(oid_str).generation_time.year\n",
    "\n",
    "    return oid_str\n",
    "\n",
    "@singleton\n",
    "class LabelBuilder(Transformer):\n",
    "    def __init__(self):\n",
    "        self._extract_year_udf = fn.udf(_extract_year_udf, t.IntegerType())('id')\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_year_from_id(oid_str):\n",
    "        return ObjectId(oid_str).generation_time.year\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('first_appear_from_id', self._extract_year_udf) \\\n",
    "            .withColumn('first_appear', fn.coalesce(fn.col('first_appear'), self._extract_year_udf))\n",
    "#         df\n",
    "#         first_appear = df.select(fn.col('first_appear')) \\\n",
    "#             .withColumn('first_appear_id', coalesce('age', 'best_guess_age')).show()\n",
    "        \n",
    "#         result = x\n",
    "#         first_appear = result.first_appear.fillna(self._extract_year(result.id.apply(ObjectId)))\n",
    "#         last_appear = result.last_available_timestamp \\\n",
    "#             .apply(self._convert_timestamp_to_coef) \\\n",
    "#             .fillna(self._extract_year(result.id.apply(ObjectId))) \\\n",
    "#             .astype(int)\n",
    "#         result.loc[:, 'label'] = last_appear - first_appear\n",
    "#         result = result[result.label.apply(lambda _x: not math.isnan(_x))]\n",
    "#         result = result[result.label >= 0]\n",
    "\n",
    "        return df\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _\n",
    "#     def bool_map(x):\n",
    "#       if x in self._bool_dict.keys():\n",
    "#         return self._bool_dict[x]\n",
    "#       return x\n",
    "#     self._bool_encode_udf = fn.udf(bool_map, t.IntegerType())\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "\n",
    "])\n",
    "\n",
    "pipe.fit(raw_data).transform(raw_data).limit(10).toPandas().loc[:,'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "@singleton\n",
    "class URLParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('url_parse_obj', fn.udf(lambda x: urlparse(x), t.UserDefinedType())('url')) \\\n",
    "            .withColumn('scheme', fn.udf(lambda x: x.scheme, t.StringType())('url_parse_obj')) \\ \n",
    "            .withColumn('netloc', fn.udf(lambda x: x.netloc, t.StringType())('url_parse_obj')) \\ \n",
    "            .withColumn('path', fn.udf(lambda x: x.path, t.StringType())('url_parse_obj')) \\ \n",
    "            .withColumn('params', fn.udf(lambda x: None if '' == x.params.strip() else x.params, t.StringType())('url_parse_obj')) \\ \n",
    "            .drop('url_parse_obj')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class URLLengthCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('url_length', fn.udf(lambda x: len(x), t.IntegerType())('url'))\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class URLDepthCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('url_depth', fn.udf(URLDepthCounter._get_depth, t.IntegerType())('path'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_depth(self, path):\n",
    "        last_idx = path.rindex('/')\n",
    "        if last_idx + 1 < len(path):\n",
    "            last_idx = len(path)\n",
    "        return path[:last_idx].count('/')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class HasWWWConverter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('has_www', fn.udf(HasWWWConverter._has_www, t.BooleanType())('netloc'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _has_www(domain):\n",
    "        return int(domain.startswith('www.'))\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SubdomainLevelCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('subdomain_level', fn.udf(SubdomainLevelCounter._get_level, t.IntegerType())('netloc'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_level(self, domain):\n",
    "        return domain.count('.')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class RequestParameterCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('params', fn.udf(RequestParameterCounter._default_blank_str, t.IntegerType())('params')) \\\n",
    "            .withColumn('param_cnt', fn.udf(RequestParameterCounter._count_param, t.IntegerType())('params'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_blank_str(self, params):\n",
    "        if np.nan == x:\n",
    "            return ''\n",
    "        return x.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_param(self, params):\n",
    "        if params is '':\n",
    "            return 0\n",
    "        return params.count('&') + 1\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "@singleton\n",
    "class DomainSuffixBuilder(Estimator, Transformer, Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._stringIndexerModel = None\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        dataset = DomainSuffixBuilder.build_suffix_port_feature(dataset)\n",
    "        \n",
    "        self._stringIndexerModel = StringIndexer(\n",
    "                inputCol=\"suffix\", outputCol=\"suffix_idx\", \n",
    "                handleInvalid=\"error\", stringOrderType=\"frequencyDesc\") \\\n",
    "            .fit(dataset)\n",
    "\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def build_suffix_port_feature(dataset):\n",
    "        return dataset.withColumn('suffix', fn.udf(DomainSuffixBuilder._get_url_suffix, t.StringType())('netloc')) \\\n",
    "            .withColumn('is_port_access', fn.udf(DomainSuffixBuilder._is_port_access, t.BooleanType())('suffix')) \\\n",
    "            .withColumn('suffix', fn.udf(DomainSuffixBuilder._clean_url_suffix, t.StringType())('suffix')) \\\n",
    "            .dropna()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_url_suffix(url):\n",
    "        if not '.' in url:\n",
    "            return None\n",
    "        last_idx = url.rindex('.')\n",
    "        return url[last_idx + 1:]\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_port_access(suffix):\n",
    "        if None is suffix:\n",
    "            return None\n",
    "        return int(len([token for token in suffix.split(':') if token.strip() != '']) > 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_url_suffix(url):\n",
    "        if None is url:\n",
    "            return None\n",
    "        return url.split(':')[0]\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = DomainSuffixBuilder.build_suffix_port_feature(dataset)\n",
    "        return self._stringIndexerModel.transform(df).na.drop(subset=['is_port_access', 'suffix', 'suffix_idx'])\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class IncorrectDomainUrlCleaner(Transformer):\n",
    "    \"\"\"\n",
    "    Remove the Incorrect Domains\n",
    "    TLD ranges from 2 to 63\n",
    "\n",
    "    Ref: https://en.wikipedia.org/wiki/Domain_Name_System#cite_ref-rfc1034_1-2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._regex = re.compile(r'^[a-zA-Z]{2,63}$', re.I)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('is_correct', fn.udf(self._is_correct, t.BooleanType())('suffix')) \\\n",
    "            .filter(fn.col('is_correct') == True) \\\n",
    "            .drop('is_correct')\n",
    "\n",
    "    def _is_correct(self, domain_suffix):\n",
    "        return True if self._regex.match(domain_suffix) else False\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class ColumnRenamer(Transformer):\n",
    "    def __init__(self, mapping):\n",
    "        self._mapping = mapping\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_mapping = {old: new for old, new in self._mapping.items() if old in df.schema.names}\n",
    "        for old, new in existing_mapping.items():\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "        \n",
    "        return df\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class BinaryNAEncoder(Transformer):\n",
    "    def __init__(self, columns):\n",
    "        self._columns = columns\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_columns = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "\n",
    "        for col_name in existing_columns:\n",
    "            df = df.withColumn(f'has_{col_name}', fn.udf(BinaryNAEncoder._encode, t.IntegerType())(col_name))\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode(x):\n",
    "        if x not in [np.nan, None]:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class EmptyHTMLFilter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.na.drop(subset=['html_text']).filter(\"html_text != ''\")\n",
    "    \n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceCodeByteCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('code_size', fn.udf(SourceCodeByteCounter._count_code_length, t.IntegerType())('html_text'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_code_length(x):\n",
    "        if x not in [np.nan, None]:\n",
    "            return len(x)\n",
    "        return 0\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class HTML5Justifier(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('is_html5', fn.udf(HTML5Justifier._is_html5, t.IntegerType())('html_text'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_html5(x):\n",
    "        if x not in [np.nan, None]:\n",
    "            is_html5 = x.replace('\\n', '') \\\n",
    "                .replace('\\r', '') \\\n",
    "                .strip() \\\n",
    "                .lower() \\\n",
    "                .startswith('<!doctype html>')\n",
    "            return 1 if is_html5 else 0\n",
    "        return 0\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class BeautifulSoupParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('bs_obj', fn.udf(\n",
    "            lambda html_doc: BeautifulSoupParser._safe_create_parser(html_doc), t.UserDefinedType()\n",
    "        )('html_text'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_create_parser(html_doc):\n",
    "        try:\n",
    "            return BeautifulSoup(html_doc, 'html.parser')\n",
    "        except:\n",
    "            return BeautifulSoup('', 'html.parser')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceTitleLengthParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('title_length', fn.udf(SourceTitleLengthParser._get_title_length, t.IntegerType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_title_length(soup):\n",
    "        \"\"\"\n",
    "        Title Length\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        title = soup.title.string if soup.title else ''\n",
    "        if not title:\n",
    "            title = ''\n",
    "        return len(title)\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceInternalJSLibCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('internal_js_cnt', fn.udf(SourceInternalJSLibCounter._count_internal_js_lib, t.IntegerType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_internal_js_lib(soup):\n",
    "        \"\"\"\n",
    "        No of internal JS files\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('script', {\"src\": True})\n",
    "        return len([0 for source in sources if not source['src'].startswith('http')])\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceExternalJSLibCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('external_js_cnt', fn.udf(SourceExternalJSLibCounter._count_external_js_lib, t.IntegerType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_external_js_lib(soup):\n",
    "        \"\"\"\n",
    "        No of external JS files\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('script', {\"src\": True})\n",
    "        return len([0 for source in sources if source['src'].startswith('http')])\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceCharsetParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('charset', fn.udf(SourceCharsetParser._get_charset, t.StringType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_charset(soup):\n",
    "        \"\"\"\n",
    "        Charset\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('meta', {\"charset\": True})\n",
    "        if 0 == len(sources):\n",
    "            return ''\n",
    "        return sources[0]['charset'].lower().replace('\\'', '').replace('\"', '')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceIFrameChecker(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('has_iframe', fn.udf(SourceIFrameChecker._has_iframe, t.BooleanType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _has_iframe(soup):\n",
    "        \"\"\"\n",
    "        iFrame in Body\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('iframe')\n",
    "        return int(0 == len(sources))\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceHyperlinkCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('has_iframe', fn.udf(SourceHyperlinkCounter._count_hyperlink, t.BooleanType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_hyperlink(soup):\n",
    "        \"\"\"\n",
    "        No of hyperlink\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('a')\n",
    "        return len([1 for source in sources if source.has_attr('href') and source['href'].lower().startswith('http')])\n",
    "\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class FeatureValueMapper(Transformer):\n",
    "    def __init__(self, column_name, mapping):\n",
    "        self._column_name = column_name\n",
    "        self._mapping = mapping\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.replace(to_replace=self._mapping, subset=[self._column_name])\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class NanToZeroConverter(Transformer):\n",
    "    def __init__(self, columns):\n",
    "        self._columns = columns\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_columns = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "\n",
    "        return df.fillna(0, subset=existing_columns)\n",
    "    \n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class FeaturePicker(Transformer):\n",
    "    def __init__(self, features):\n",
    "        self._features = features\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_columns = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "        \n",
    "        return df.select(*existing_columns)\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class DummySuffixDescritizer(Transformer):\n",
    "    def __init__(self, features):\n",
    "        self._features = features\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        categories = df.select(\"suffix\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        exprs = [\n",
    "            fn.when(F.col(\"suffix\") == category, 1).otherwise(0).alias(category)\n",
    "            for category in categories\n",
    "        ]\n",
    "        dummy_df = df.select(fn.col('id'), fn.('suffix')).select('id', *exprs)\n",
    "\n",
    "        return df.join(dummy_df, 'id', 'inner').drop('suffix')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class FeatureRemover(Transformer):\n",
    "    def __init__(self, features):\n",
    "        self._features = features\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        removed_features = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "        return df.drop(*removed_features)\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "    FeatureRemover([\n",
    "        'is_port_access',\n",
    "    ]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "    FeatureRemover([\n",
    "        'is_port_access',\n",
    "    ]),\n",
    "    StringIndexer(\n",
    "                inputCol=\"charset\", outputCol=\"charset\", \n",
    "                handleInvalid=\"error\", stringOrderType=\"frequencyDesc\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class CustomizedStandardizer(Transformer):\n",
    "    def __init__(self, norm='l2'):\n",
    "        self._pipe = Pipeline([\n",
    "            ('standard_scaler', preprocessing.StandardScaler()),\n",
    "\n",
    "        ])\n",
    "        self._columns = None\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        \n",
    "        df_unique = df.agg(*(fn.countDistinct(fn.col(col_name)).alias(col_name) for col_name in df.schema.names))\n",
    "        \n",
    "        \n",
    "        return df\n",
    "\n",
    "    \n",
    "# class CustomizedStandardizer(BaseEstimator, TransformerMixin):\n",
    "#     \"\"\"\n",
    "#     Add Sklearn Build-in Function\n",
    "#     \"\"\"\n",
    "#     def __init__(self, norm='l2'):\n",
    "#         self._pipe = Pipeline([\n",
    "#             ('standard_scaler', preprocessing.StandardScaler()),\n",
    "\n",
    "#         ])\n",
    "#         self._columns = None\n",
    "\n",
    "#     @property\n",
    "#     def columns(self):\n",
    "#         return self._columns\n",
    "\n",
    "#     def fit(self,x,y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self,x,y=None):\n",
    "#         result = x\n",
    "\n",
    "#         df_unique = pd.DataFrame()\n",
    "#         for col_name in result.drop('label', axis=1).columns:\n",
    "#             df_unique[col_name] = [len(result[col_name].unique())]\n",
    "\n",
    "#         df_unique.index = ['unique count']\n",
    "#         df_unique = df_unique.T.squeeze()\n",
    "\n",
    "#         binary_columns = df_unique[df_unique < 3].index.tolist()\n",
    "#         numeric_columns = x.drop([*binary_columns, 'label'], axis=1).select_dtypes(include=np.number).columns.tolist()\n",
    "#         other_columns = x.drop([*binary_columns, *numeric_columns, 'label'], axis=1).columns.tolist()\n",
    "#         label = x.label.tolist()\n",
    "#         label = np.array([label]).T\n",
    "\n",
    "#         result = label\n",
    "#         if len(binary_columns) > 0:\n",
    "#             result = np.append(result, x[binary_columns], axis=1)\n",
    "#         if len(numeric_columns) > 0:\n",
    "#             numeric_result = self._pipe.fit_transform(x[numeric_columns])\n",
    "#             result = np.append(result, numeric_result, axis=1)\n",
    "#         if len(other_columns) > 0:\n",
    "#             result = np.append(result, x[other_columns], axis=1)\n",
    "\n",
    "#         result = pd.DataFrame(result, columns= ['label', *binary_columns, *numeric_columns, *other_columns])\n",
    "#         self._columns = [*binary_columns, *numeric_columns, *other_columns, 'label']\n",
    "\n",
    "# #         result.loc[:, 'label'] = x.label-1970\n",
    "#         return result[self._columns]\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "    FeatureRemover([\n",
    "        'is_port_access',\n",
    "    ]),\n",
    "    StringIndexer(\n",
    "                inputCol=\"charset\", outputCol=\"charset\", \n",
    "                handleInvalid=\"error\", stringOrderType=\"frequencyDesc\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.limit(2000).toPandas().to_json('tmp_spark.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_json('tmp_spark.json', orient='index')\n",
    "\n",
    "raw_data = spark.createDataFrame(raw_data)\n",
    "\n",
    "shape(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6th Edition - Combine suffix dummy with MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import categorical_encoders\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('label_builder', TobitLabelBuilder()),\n",
    "    ('url_parser', URLParser()),\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('binary_na_encoder', BinaryNAEncoder(['content_type'])),\n",
    "    ('html_parser', html_parser),\n",
    "    ('binary_feature_converter', FeatureValueMapper('protocol_type', {\n",
    "                                        'http': 1,\n",
    "                                        'https':0,\n",
    "                                        })),\n",
    "\n",
    "    ('nan_to_Zero_converter', NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation'\n",
    "    ])),\n",
    "    \n",
    "    ('feature_picker', FeaturePicker([\n",
    "                                        'protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix_idx',\n",
    "                                        'is_port_access',\n",
    "                                        'code_size',\n",
    "                                        'title_length',\n",
    "                                        'internal_js_cnt',\n",
    "                                        'external_js_cnt',\n",
    "                                        'charset',\n",
    "                                        'is_html5',\n",
    "                                        'has_iframe',\n",
    "                                        'hyperlink_cnt',\n",
    "                                        'first_appear',\n",
    "\n",
    "                                        'total_num_of_paper_citing',\n",
    "                                        'total_num_of_author_citing',\n",
    "                                        'total_num_of_affiliation_citing',\n",
    "                                        'total_num_of_journal_citing',\n",
    "                                        'total_num_of_author_self_citation',\n",
    "                                        'total_num_of_affiliation_self_citation',\n",
    "                                        'total_num_of_journal_self_citation',\n",
    "                                        'avg_year',\n",
    "                                        'min_year',\n",
    "                                        'max_year',\n",
    "                                        'median',\n",
    "                                        'num_of_author',\n",
    "                                        'num_of_author_citing',\n",
    "                                        'num_of_affiliation_citing',\n",
    "                                        'num_of_journal_citing',\n",
    "                                        'avg_hindex',\n",
    "                                        'first_author_hindex',\n",
    "                                        'last_author_hindex',\n",
    "                                        'avg_mid_author_hindex',\n",
    "                                        'paper_unique_affiliation',\n",
    "\n",
    "                                        'label',\n",
    "                                       ])),\n",
    "    ('dummy_suffix_descritizer', DummySuffixDescritizer()),\n",
    "\n",
    "    ('feature_remover', FeatureRemover([\n",
    "                                        'is_port_access',\n",
    "                                       ])),\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['charset'])),\n",
    "    ('standard_scaler', TobitCustomizedStandardizer(norm='l2')),\n",
    "\n",
    "])\n",
    "\n",
    "pipe.fit_transform(DataSource().raw_data).to_csv('untrunc_data_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
