{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "        sys.path.append(path)\n",
    "add_path('/home/jjian03/anaconda3/lib/python3.7/site-packages')\n",
    "add_path(f'{os.path.abspath(os.path.join(\".\"))}/lib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated 16 CPUs\n"
     ]
    }
   ],
   "source": [
    "from lib.Repository import *\n",
    "from lib.Utility import *\n",
    "from lib.modeling import *\n",
    "from lib.preprocessing import *\n",
    "from lib.preprocessing.HTMLParser import html_parser\n",
    "from lib.viz import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark Session....\n",
      "Adding Libraries /home/jjian03/lib/bson.zip\n",
      "2.3.0.cloudera3\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(spark, path, name):\n",
    "    return spark.read.parquet(path).registerTempTable(name)\n",
    "\n",
    "def shape(df):\n",
    "    print((df.count(), len(df.columns)))\n",
    "\n",
    "def create_session(libraries_list):\n",
    "    print('Creating Spark Session....')\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "            .config('spark.app.name', 'Tobit regression') \\\n",
    "            .config('spark.dynamicAllocation.enabled', 'true') \\\n",
    "            .config('spark.dynamicAllocation.maxExecutors', '50') \\\n",
    "            .config('spark.dynamicAllocation.executorIdleTimeout', '90s') \\\n",
    "            .config('spark.driver.maxResultSize', '20g') \\\n",
    "            .config('spark.driver.memory', '50g') \\\n",
    "            .config('spark.executor.memory', '20g') \\\n",
    "            .config('spark.task.maxFailures', '3') \\\n",
    "            .config('spark.yarn.am.memory', '50g') \\\n",
    "            .config('spark.yarn.max.executor.failures', '3') \\\n",
    "            .config('spark.kryoserializer.buffer.max', '1024m') \\\n",
    "            .config('spark.yarn.executor.memoryOverhead', '50g') \\\n",
    "            .getOrCreate()\n",
    "\n",
    "#             .config('spark.cores.max', 8) \\\n",
    "#             .config('spark.executor.cores', 8) \\\n",
    "#             .config(\"spark.executor.instances\", 8) \\\n",
    "#             .config('spark.executorEnv.PYTHON_EGG_CACHE', '/home/jjian03/cache') \\\n",
    "\n",
    "    for library in libraries_list:\n",
    "        print('Adding Libraries ' + str(library))\n",
    "        spark.sparkContext.addPyFile(library)    # adding libraries\n",
    "#         spark.sparkContext.addPyFile(library)\n",
    "    return spark\n",
    "\n",
    "try:\n",
    "    print(spark.version)\n",
    "except NameError as e:\n",
    "    from pyspark.sql import SparkSession\n",
    "    libraries_list = [\n",
    "        '/home/jjian03/lib/bson.zip',\n",
    "#         '/home/jjian03/lib/pymongo.zip',\n",
    "#         '/home/nsf_data_ingestion/libraries/pubmed_parser_lib.zip',\n",
    "#         '/home/nsf_data_ingestion/libraries/unidecode_lib.zip'\n",
    "    ]\n",
    "    spark = create_session(libraries_list)\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    spark_sql = SQLContext(sc)\n",
    "    print(spark.version)\n",
    "\n",
    "    load_dataset(spark, '/user/jjian03/WebResourceQuality.parquet', 'web_resource_quality')\n",
    "    load_dataset(spark, '/user/jjian03/WebResourceQuality_pmid.parquet', 'web_resource_quality_pmid')\n",
    "    load_dataset(spark, '/datasets/MAG_20200403/MAG_Azure_Parquet/mag_parquet/Papers.parquet', 'Paper')\n",
    "    load_dataset(spark, '/user/lliang06/icon/MAG_publication_features.parquet', 'mag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as t\n",
    "from bson import ObjectId\n",
    "import bson\n",
    "\n",
    "\n",
    "df = sc.parallelize([('5ecd87e7150a1889d703ea37', 2.0), ('5ecd87e7150a1889d703ea37', 3.0)]).toDF([\"id\", \"x\"])\n",
    "def _extract_year_udf(oid_str):\n",
    "#         return ObjectId(oid_str).generation_time.year\n",
    "    return ObjectId(oid_str).generation_time.year\n",
    "\n",
    "    return oid_str\n",
    "df.withColumn('y', fn.udf(_extract_year_udf, t.StringType())('id')).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://github.com/mongodb/mongo-python-driver/archive/master.zip --output pymongo.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fract = 0.003\n",
    "\n",
    "raw_data = spark_sql.sql(f'''\n",
    "        SELECT wr.id\n",
    "            , wr.url\n",
    "            , wr.actual_scrape_url\n",
    "            , wr.first_appear\n",
    "            , wr.first_available_timestamp\n",
    "            , wr.last_available_timestamp\n",
    "            , wr.header\n",
    "            , wr.html_text\n",
    "            , wr.comment\n",
    "            , wr.from_waybackmachine\n",
    "            , wr.http_status_code\n",
    "            , wr.original_check_failure\n",
    "            , wr.original_check_error_log\n",
    "            , wr.terminate_reason\n",
    "            , wr.terminate_reason_error_log\n",
    "\n",
    "            , m.paperId\n",
    "            , m.total_num_of_paper_citing\n",
    "            , m.total_num_of_author_citing\n",
    "            , m.total_num_of_affiliation_citing\n",
    "            , m.total_num_of_journal_citing\n",
    "            , m.total_num_of_author_self_citation\n",
    "            , m.total_num_of_affiliation_self_citation\n",
    "            , m.total_num_of_journal_self_citation\n",
    "            , m.avg_year\n",
    "            , m.min_year\n",
    "            , m.max_year\n",
    "            , m.median\n",
    "            , m.num_of_author\n",
    "            , m.num_of_author_citing\n",
    "            , m.num_of_affiliation_citing\n",
    "            , m.num_of_journal_citing\n",
    "            , m.avg_hindex\n",
    "            , m.first_author_hindex\n",
    "            , m.last_author_hindex\n",
    "            , m.avg_mid_author_hindex\n",
    "            , m.paper_unique_affiliation\n",
    "\n",
    "            , m.paper_unique_affiliation\n",
    "\n",
    "        FROM web_resource_quality wr\n",
    "        JOIN web_resource_quality_pmid wr_doi ON wr.id = wr_doi.id\n",
    "        JOIN Paper p ON wr_doi.doi = p.doi\n",
    "        JOIN mag m ON p.paperId = m.paperId\n",
    "        WHERE wr.label IS NOT NULL\n",
    "        AND wr.label IN ('0', '1')\n",
    "        AND isNaN(wr.label) = false\n",
    "        AND wr.first_appear IS NOT NULL\n",
    "        AND isNaN(wr.first_appear) = false\n",
    "        AND lower(wr.url) NOT LIKE \"%doi.org%\"\n",
    "    ''') \\\n",
    "    .orderBy(fn.rand(seed=seed)) \\\n",
    "    .sample(False, fract, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "start_time = time.time()\n",
    "\n",
    "# raw_data.printSchema()\n",
    "\n",
    "print(f'raw_data: {shape(raw_data)}')\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Tobit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml.regression import Params, HasRegParam, HasElasticNetParam, HasMaxIter, Param, \\\n",
    "    TypeConverters, HasInputCol, HasRawPredictionCol, HasPredictionCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.sql.functions import rand\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class HasLogSigma(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param log sigma: log sigma names.\n",
    "    \"\"\"\n",
    "\n",
    "    logSigma = Param(Params._dummy(), \"logSigma\", \"log sigma names.\", typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasLogSigma, self).__init__()\n",
    "\n",
    "    def setLogSigma(self, value):\n",
    "        return self._set(logSigma=value)\n",
    "\n",
    "    def getLogSigma(self):\n",
    "        \"\"\"\n",
    "        Gets the value of log sigma or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.logSigma)\n",
    "\n",
    "\n",
    "class HasLearningRate(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param learningRate: learning rate of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    learningRate = Param(Params._dummy(), \"learningRate\", \"Learning Rate of the model.\", typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasLearningRate, self).__init__()\n",
    "\n",
    "    def setLearningRate(self, value):\n",
    "        return self._set(learningRate=value)\n",
    "\n",
    "    def getLearningRate(self):\n",
    "        \"\"\"\n",
    "        Gets the value of learningRate or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.learningRate)\n",
    "\n",
    "\n",
    "class HasCoefficients(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param coefficients: coefficients of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    coefficients = Param(Params._dummy(), \"coefficients\", \"Coefficients of the model.\", typeConverter=TypeConverters.toList)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasCoefficients, self).__init__()\n",
    "\n",
    "    def setCoefficients(self, value):\n",
    "        return self._set(coefficients=value)\n",
    "\n",
    "    def getCoefficients(self):\n",
    "        \"\"\"\n",
    "        Gets the value of coefficients or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.coefficients)\n",
    "\n",
    "\n",
    "class _LassoTobitParameter(HasCoefficients, HasLogSigma, HasRegParam, HasElasticNetParam, HasMaxIter):\n",
    "\n",
    "    leftCensorPoint = Param(Params._dummy(), \"leftCensorPoint\",\n",
    "                   \"Censored threshold on the left hand side\",\n",
    "                   typeConverter=TypeConverters.toFloat)\n",
    "    rightCensorPoint = Param(Params._dummy(), \"rightCensorPoint\",\n",
    "                   \"Censored threshold on the right hand side\",\n",
    "                   typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def setLeftCensorPoint(self, value):\n",
    "        return self._set(leftCensorPoint=value)\n",
    "\n",
    "    def getLeftCensorPoint(self):\n",
    "        \"\"\"\n",
    "        Gets the value of :py:attr:`leftCensorPoint` or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.leftCensorPoint)\n",
    "\n",
    "    def setRightCensorPoint(self, value):\n",
    "        return self._set(rightCensorPoint=value)\n",
    "\n",
    "    def getRightCensorPoint(self):\n",
    "        \"\"\"\n",
    "        Gets the value of :py:attr:`rightCensorPoint` or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.rightCensorPoint)\n",
    "\n",
    "\n",
    "def _parallelFitTasks(est, train, eva, validation, epm):\n",
    "    \"\"\"\n",
    "    Creates a list of callables which can be called from different threads to fit and evaluate\n",
    "    an estimator in parallel. Each callable returns an `(index, metric)` pair.\n",
    "\n",
    "    :param est: Estimator, the estimator to be fit.\n",
    "    :param train: DataFrame, training data set, used for fitting.\n",
    "    :param eva: Evaluator, used to compute `metric`\n",
    "    :param validation: DataFrame, validation data set, used for evaluation.\n",
    "    :param epm: Sequence of ParamMap, params maps to be used during fitting & evaluation.\n",
    "    :return: (int, float), an index into `epm` and the associated metric value.\n",
    "    \"\"\"\n",
    "    modelIter = est.fitMultiple(train, epm)\n",
    "\n",
    "    def singleTask():\n",
    "        index, model = next(modelIter)\n",
    "        eva_copy = copy.copy(eva)\n",
    "        eva_copy.model = model.coefficients\n",
    "        metric = eva.evaluate(model.transform(validation, epm[index]))\n",
    "        return index, metric\n",
    "\n",
    "    return [singleTask] * len(epm)\n",
    "\n",
    "\n",
    "\n",
    "class TobitCrossValidator(CrossValidator):\n",
    "    \"\"\"\n",
    "    Avoid mulitple calculation on coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3,\n",
    "                 seed=None, parallelism=1, collectSubModels=False):\n",
    "        \"\"\"\n",
    "        __init__(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3,\\\n",
    "                 seed=None, parallelism=1, collectSubModels=False)\n",
    "        \"\"\"\n",
    "        super(TobitCrossValidator, self).__init__()\n",
    "        self._setDefault(numFolds=3, parallelism=1)\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        nFolds = self.getOrDefault(self.numFolds)\n",
    "        seed = self.getOrDefault(self.seed)\n",
    "        h = 1.0 / nFolds\n",
    "        randCol = self.uid + \"_rand\"\n",
    "        df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
    "        metrics = [0.0] * numModels\n",
    "\n",
    "        pool = ThreadPool(processes=min(self.getParallelism(), numModels))\n",
    "\n",
    "        for i in range(nFolds):\n",
    "            validateLB = i * h\n",
    "            validateUB = (i + 1) * h\n",
    "            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n",
    "            validation = df.filter(condition).cache()\n",
    "            train = df.filter(~condition).cache()\n",
    "\n",
    "            tasks = _parallelFitTasks(est, train, eva, validation, epm)\n",
    "            for j, metric in pool.imap_unordered(lambda f: f(), tasks):\n",
    "                metrics[j] += (metric / nFolds)\n",
    "            validation.unpersist()\n",
    "            train.unpersist()\n",
    "\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "        bestModel = est.fit(dataset, epm[bestIndex])\n",
    "        return self._copyValues(CrossValidatorModel(bestModel, metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoTobitRegression(Estimator, HasInputCol, HasRawPredictionCol, HasPredictionCol,\n",
    "                           HasLearningRate, _LassoTobitParameter,\n",
    "                           DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, rawPredictionCol=None, predictionCol=None,\n",
    "                 coefficients: list=None, logSigma: float=None,\n",
    "                 regParam: float=None, elasticNetParam: float=None, maxIter: int=None,\n",
    "                 leftCensorPoint: float=None, rightCensorPoint: float=None,\n",
    "                 learningRate: float = None,\n",
    "                 ):\n",
    "        super(LassoTobitRegression, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, rawPredictionCol=None, predictionCol=None,\n",
    "                  coefficients: list=None, logSigma: float=None,\n",
    "                  regParam: float = None, elasticNetParam: float = None, maxIter: int = None,\n",
    "                  leftCensorPoint: float = None, rightCensorPoint: float = None,\n",
    "                  learningRate: float = None,\n",
    "                  ):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        X = self.getInputCol()\n",
    "        y = self.getRawPredictionCol()\n",
    "        lbd = self.getRegParam()\n",
    "        alpha = self.getElasticNetParam()\n",
    "        maxIter = self.getMaxIter()\n",
    "        left = self.getLeftCensorPoint()\n",
    "        right = self.getRightCensorPoint()\n",
    "        learningRate = self.getLearningRate()\n",
    "\n",
    "        # initialize with OLS\n",
    "        coef = self.getCoefficients()\n",
    "        if len(coef) != len(dataset.schema.names):\n",
    "            print('Column does not match!')\n",
    "            print(*coef, sep='\\n')\n",
    "            print('------')\n",
    "            print(*dataset.schema.names, sep='\\n')\n",
    "            raise AssertionError('Column does not match!')\n",
    "        logSigma = self.getLogSigma()\n",
    "\n",
    "        gradient = \n",
    "        print('шонч╗Г')\n",
    "        coefficients = None\n",
    "        \n",
    "        return LassoTobitRegressionModel(\n",
    "            inputCol=c, predictionCol=self.getPredictionCol(),\n",
    "            regParam=self.getRegParam(),\n",
    "            elasticNetParam=self.getElasticNetParam(),\n",
    "            maxIter=self.getMaxIter(),\n",
    "            leftCensorPoint=self.getLeftCensorPoint(),\n",
    "            rightCensorPoint=self.getRightCensorPoint(),\n",
    "            coefficients=coefficients,\n",
    "            logSigma=self.getLogSigma(),\n",
    "        )\n",
    "\n",
    "\n",
    "class LassoTobitRegressionModel(Model,\n",
    "                                HasInputCol, HasRawPredictionCol, HasPredictionCol,\n",
    "                                _LassoTobitParameter,\n",
    "                                DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, predictionCol=None,\n",
    "                 coefficients: list=None, logSigma: float=None,\n",
    "                 regParam=None, elasticNetParam=None, maxIter=None,\n",
    "                 leftCensorPoint=None, rightCensorPoint=None,\n",
    "                 ):\n",
    "        super(LassoTobitRegressionModel, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, predictionCol=None,\n",
    "                  regParam=None, elasticNetParam=None, maxIter=None,\n",
    "                  leftCensorPoint=None, rightCensorPoint=None,\n",
    "                  coefficients=None,\n",
    "                  ):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        x = self.getInputCol()\n",
    "        y = self.getPredictionCol()\n",
    "        coefficients = self.getCoefficients()\n",
    "\n",
    "        # Predict\n",
    "        # return dataset.withColumn(y, (dataset[x] - mu) > threshold * sigma)\n",
    "        return dataset.withColumn('Clever', fn.col('x'))\n",
    "\n",
    "\n",
    "# Test\n",
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "class LassoTobitEvaluator(Evaluator, HasInputCol, HasRawPredictionCol, HasPredictionCol):\n",
    "\n",
    "    def __init__(self, inputCol='features', predictionCol=\"prediction\", rawPredictionCol=\"rawPredictionCol\",\n",
    "                 model: Model=None,\n",
    "                 ):\n",
    "        self.inputCol = inputCol\n",
    "        self.predictionCol = predictionCol\n",
    "        self.rawPredictionCol = rawPredictionCol\n",
    "        self.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def _censored_udf(x):\n",
    "        pass\n",
    "\n",
    "    def _calculate_loglik():\n",
    "        pass\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def _evaluate(self, dataset):\n",
    "        \"\"\"\n",
    "        Returns a random number.\n",
    "        Implement here the true metric\n",
    "        \"\"\"\n",
    "        # calculate loglik\n",
    "        ll = self.model.getLeftCensorPoint()\n",
    "        rl = self.model.getRightCensorPoint()\n",
    "        logSigma = self.model.getLogSigma()\n",
    "        label = dataset.select(udf(LassoTobitEvaluator._censored_udf(self.getPredictionCol()), IntegerType()))\n",
    "        X = dataset.withColumn('intercept', fn.lit(1)) \\\n",
    "            .select([fn.col(col_name) for col_name in ['intercept', *self.getInputCol()]])\n",
    "        y = dataset.select(fn.col(self.getRawPredictionCol()))\n",
    "        coef = model.getCoefficients()\n",
    "        \n",
    "        xb = X@coef\n",
    "        \n",
    "        X = X.select(fn.col(col_names[len(col_names)-1]))\n",
    "        col_names = dataset.schema.names\n",
    "        label_udf = udf(lambda x: ,FloatType())\n",
    "          uncensored <- sum(log(dnorm(((Y[which(I>ll & I <ul)] - xb[which(I>ll & I <ul)])/ sigma), mean = 0, sd = 1)) - log(sigma))\n",
    "\n",
    "          # The alive resources are considered to be censored. They are labeled as 1\n",
    "          # Only right censored term applied for our case\n",
    "          ll_censored <- sum(log(1-pnorm((xb[which(I<=ll)]) / sigma, mean = 0, sd = 1)))\n",
    "          ul_censored <- sum(log(1-pnorm((ul - xb[which(I>=ul)]) / sigma, mean = 0, sd = 1)))\n",
    "        return ll_censored + uncensored + ul_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "df = sc.parallelize([(1, 2.0), (2, 3.0), (3, 0.0), (4, 99.0)]).toDF([\"id\", \"x\"])\n",
    "\n",
    "lasso_tobit_regressor = LassoTobitRegression() \\\n",
    "    .setInputCol(\"x\") \\\n",
    "    .setLeftCensorPoint(0) \\\n",
    "    .setRightCensorPoint(100) \\\n",
    "    .setRegParam(1) \\\n",
    "    .setMaxIter(100) \\\n",
    "    .setElasticNetParam(1)\n",
    "pipe  = Pipeline(stages=[lasso_tobit_regressor])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lasso_tobit_regressor.leftCensorPoint, [0]) \\\n",
    "    .addGrid(lasso_tobit_regressor.rightCensorPoint, [100]) \\\n",
    "    .addGrid(lasso_tobit_regressor.regParam, [1]) \\\n",
    "    .addGrid(lasso_tobit_regressor.maxIter, [100]) \\\n",
    "    .addGrid(lasso_tobit_regressor.elasticNetParam, [1]) \\\n",
    "    .build()\n",
    "\n",
    "# evaluator = LassoTobitEvaluator(labelCol='price')\n",
    "evaluator = LassoTobitEvaluator()\n",
    "\n",
    "# import inspect\n",
    "\n",
    "# print(inspect.getsource(CrossValidator()._fit))\n",
    "crossval = TobitCrossValidator() \\\n",
    "    .setEstimator(pipe) \\\n",
    "    .setEstimatorParamMaps(paramGrid) \\\n",
    "    .setEvaluator(evaluator) \\\n",
    "    .setNumFolds(2)\n",
    "# crossval = TobitCrossValidator(estimator=pipe,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=2)\n",
    "cvModel = crossval.fit(df)\n",
    "bestModel = cvModel.bestModel\n",
    "preds = bestModel.transform(df)\n",
    "\n",
    "preds.show()\n",
    "\n",
    "# cvModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "\n",
    "@singleton\n",
    "class LabelBuilder(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumnRenamed('from_waybackmachine', 'label') \\\n",
    "            .withColumn('label', fn.udf(lambda x: 0 if x else 1, t.IntegerType())('label'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "\n",
    "from bson import ObjectId\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@singleton\n",
    "class LastAppearBuilder(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _extract_year_in_id(self, x):\n",
    "        return ObjectId(x).generation_time.year\n",
    "\n",
    "    def _convert_timestamp_to_coef(self, ts):\n",
    "        if None is ts or np.nan is ts or math.isnan(ts):\n",
    "            return ts\n",
    "        ts_str = str(ts).strip()\n",
    "        if '' == ts_str:\n",
    "            return ts\n",
    "\n",
    "        ts_str = str(int(float(ts_str)))\n",
    "        ts_obj = datetime.datetime.strptime(ts_str, \"%Y%m%d%H%M%S\")\n",
    "        return ts_obj.year\n",
    "\n",
    "    def _adjust_1990(self, last_appear):\n",
    "        return last_appear - 1990\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('year_in_id', fn.udf(self._extract_year_in_id, t.IntegerType())('id')) \\\n",
    "            .withColumn('last_appear', fn.udf(self._convert_timestamp_to_coef, t.IntegerType())('last_available_timestamp')) \\\n",
    "            .withColumn('last_appear', fn.coalesce('last_appear', 'year_in_id')) \\\n",
    "            .withColumn('last_appear', fn.udf(self._adjust_1990, t.IntegerType())('last_appear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "@singleton\n",
    "class URLParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('url_parse_obj', fn.udf(lambda x: urlparse(x))('url')) \\\n",
    "            .withColumn('scheme', fn.udf(lambda x: x.scheme, t.StringType())('url_parse_obj')) \\\n",
    "            .withColumn('netloc', fn.udf(lambda x: x.netloc, t.StringType())('url_parse_obj')) \\\n",
    "            .withColumn('path', fn.udf(lambda x: x.path, t.StringType())('url_parse_obj')) \\\n",
    "            .withColumn('params', fn.udf(lambda x: None if '' == x.params.strip() else x.params, t.StringType())('url_parse_obj')) \\\n",
    "            .drop('url_parse_obj')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class URLLengthCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('url_length', fn.udf(lambda x: len(x), t.IntegerType())('url'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class URLDepthCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('url_depth', fn.udf(self._get_depth, t.IntegerType())('path'))\n",
    "\n",
    "    def _get_depth(self, path):\n",
    "        last_idx = path.rindex('/')\n",
    "        if last_idx + 1 < len(path):\n",
    "            last_idx = len(path)\n",
    "        return path[:last_idx].count('/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class HasWWWConverter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('has_www', fn.udf(self._has_www, t.IntegerType())('netloc'))\n",
    "\n",
    "    def _has_www(self, domain):\n",
    "        return int(domain.startswith('www.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SubdomainLevelCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('subdomain_level', fn.udf(self._get_level, t.IntegerType())('netloc'))\n",
    "\n",
    "    def _get_level(self, domain):\n",
    "        return domain.count('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class RequestParameterCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('params', fn.udf(self._default_blank_str, t.IntegerType())('params')) \\\n",
    "            .withColumn('param_cnt', fn.udf(self._count_param, t.IntegerType())('params'))\n",
    "\n",
    "    def _default_blank_str(self, params):\n",
    "        if np.nan == params or not params:\n",
    "            return ''\n",
    "        return params.strip()\n",
    "\n",
    "    def _count_param(self, params):\n",
    "        if params is '':\n",
    "            return 0\n",
    "        return params.count('&') + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "class DomainSuffixBuilder(Transformer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._stringIndexerModel = None\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        dataset = self.build_suffix_port_feature(dataset)\n",
    "        \n",
    "        self._stringIndexerModel = StringIndexer(\n",
    "                inputCol=\"suffix\", outputCol=\"suffix_idx\", \n",
    "                handleInvalid=\"error\", stringOrderType=\"frequencyDesc\") \\\n",
    "            .fit(dataset)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def build_suffix_port_feature(self, dataset):\n",
    "        return dataset.withColumn('suffix', fn.udf(self._get_url_suffix, t.StringType())('netloc')) \\\n",
    "            .withColumn('is_port_access', fn.udf(self._is_port_access, t.IntegerType())('suffix')) \\\n",
    "            .withColumn('suffix', fn.udf(self._clean_url_suffix, t.StringType())('suffix')) \\\n",
    "            .filter(fn.col('suffix').isNotNull()) \\\n",
    "            .filter(fn.col('is_port_access').isNotNull())\n",
    "\n",
    "    def _get_url_suffix(self, url):\n",
    "        if not '.' in url:\n",
    "            return None\n",
    "        last_idx = url.rindex('.')\n",
    "        return url[last_idx + 1:]\n",
    "\n",
    "    def _is_port_access(self, suffix):\n",
    "        if None is suffix:\n",
    "            return 0\n",
    "        return int(len([token for token in suffix.split(':') if token.strip() != '']) > 1)\n",
    "\n",
    "    def _clean_url_suffix(self, url):\n",
    "        if None is url:\n",
    "            return None\n",
    "        return url.split(':')[0]\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = self.build_suffix_port_feature(df)\n",
    "        if not self._stringIndexerModel:\n",
    "            self._stringIndexerModel = StringIndexer(\n",
    "                    inputCol=\"suffix\", outputCol=\"suffix_idx\", \n",
    "                    handleInvalid=\"error\", stringOrderType=\"frequencyDesc\") \\\n",
    "                .fit(df)\n",
    "        return self._stringIndexerModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "@singleton\n",
    "class IncorrectDomainUrlCleaner(Transformer):\n",
    "    \"\"\"\n",
    "    Remove the Incorrect Domains\n",
    "    TLD ranges from 2 to 63\n",
    "\n",
    "    Ref: https://en.wikipedia.org/wiki/Domain_Name_System#cite_ref-rfc1034_1-2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._regex = re.compile(r'^[a-zA-Z]{2,63}$', re.I)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('is_correct', fn.udf(self._is_correct, t.BooleanType())('suffix')) \\\n",
    "            .filter(fn.col('is_correct') == True) \\\n",
    "            .drop('is_correct')\n",
    "\n",
    "    def _is_correct(self, domain_suffix):\n",
    "        return True if self._regex.match(domain_suffix) else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class ColumnRenamer(Transformer):\n",
    "    def __init__(self, mapping):\n",
    "        self._mapping = mapping\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_mapping = {old: new for old, new in self._mapping.items() if old in df.schema.names}\n",
    "        for old, new in existing_mapping.items():\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No content type in the dataset, will investigate it later\n",
    "\n",
    "@singleton\n",
    "class BinaryNAEncoder(Transformer):\n",
    "    def __init__(self, columns):\n",
    "        self._columns = columns\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_columns = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "\n",
    "        for col_name in existing_columns:\n",
    "            df = df.withColumn(f'has_{col_name}', fn.udf(BinaryNAEncoder._encode, t.IntegerType())(col_name))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode(x):\n",
    "        if x not in [np.nan, None]:\n",
    "            return 1\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class EmptyHTMLFilter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.filter(\"html_text != ''\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceCodeByteCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('code_size', fn.udf(self._count_code_length, t.IntegerType())('html_text'))\n",
    "\n",
    "    def _count_code_length(self, x):\n",
    "        if x not in [np.nan, None]:\n",
    "            return len(x)\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_html5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_html5\n",
       "0         1\n",
       "1         0\n",
       "2         0\n",
       "3         1\n",
       "4         0\n",
       "5         1\n",
       "6         1\n",
       "7         1\n",
       "8         1\n",
       "9         0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@singleton\n",
    "class HTML5Justifier(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('is_html5', fn.udf(self._is_html5, t.IntegerType())('html_text'))\n",
    "\n",
    "    def _is_html5(self, x):\n",
    "        if x not in [np.nan, None]:\n",
    "            is_html5 = x.replace('\\n', '') \\\n",
    "                .replace('\\r', '') \\\n",
    "                .strip() \\\n",
    "                .lower() \\\n",
    "                .startswith('<!doctype html>')\n",
    "            return 1 if is_html5 else 0\n",
    "        return 0\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "])\n",
    "\n",
    "pipe.fit(raw_data).transform(raw_data).limit(10).toPandas().loc[:,['is_html5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3446.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 3 times, most recent failure: Lost task 0.2 in stage 14.0 (TID 25, ist-deacuna-n4.syr.edu, executor 3): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:333)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:322)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:88)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:73)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:333)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:322)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:88)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:73)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7963dbd6a58a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m ])\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bs_obj'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m             \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3446.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 3 times, most recent failure: Lost task 0.2 in stage 14.0 (TID 25, ist-deacuna-n4.syr.edu, executor 3): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:333)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:322)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:88)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:73)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:333)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:322)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:88)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:73)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "@singleton\n",
    "class BeautifulSoupParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('bs_obj', fn.udf(\n",
    "            lambda html_doc: self._safe_create_parser(html_doc)\n",
    "        )('html_text'))\n",
    "\n",
    "    def _safe_create_parser(self, html_doc):\n",
    "        try:\n",
    "            import faulthandler\n",
    "            faulthandler.enable()\n",
    "            import sys\n",
    "            sys.setrecursionlimit(1000000)\n",
    "            return BeautifulSoup(html_doc, 'html.parser')\n",
    "        except:\n",
    "            return BeautifulSoup('', 'html.parser')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "])\n",
    "\n",
    "pipe.fit(raw_data).transform(raw_data).limit(10).toPandas().loc[:,['bs_obj']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceTitleLengthParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('title_length', fn.udf(SourceTitleLengthParser._get_title_length, t.IntegerType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_title_length(soup):\n",
    "        \"\"\"\n",
    "        Title Length\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        title = soup.title.string if soup.title else ''\n",
    "        if not title:\n",
    "            title = ''\n",
    "        return len(title)\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceInternalJSLibCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('internal_js_cnt', fn.udf(SourceInternalJSLibCounter._count_internal_js_lib, t.IntegerType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_internal_js_lib(soup):\n",
    "        \"\"\"\n",
    "        No of internal JS files\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('script', {\"src\": True})\n",
    "        return len([0 for source in sources if not source['src'].startswith('http')])\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceExternalJSLibCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('external_js_cnt', fn.udf(SourceExternalJSLibCounter._count_external_js_lib, t.IntegerType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_external_js_lib(soup):\n",
    "        \"\"\"\n",
    "        No of external JS files\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('script', {\"src\": True})\n",
    "        return len([0 for source in sources if source['src'].startswith('http')])\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceCharsetParser(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('charset', fn.udf(SourceCharsetParser._get_charset, t.StringType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_charset(soup):\n",
    "        \"\"\"\n",
    "        Charset\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('meta', {\"charset\": True})\n",
    "        if 0 == len(sources):\n",
    "            return ''\n",
    "        return sources[0]['charset'].lower().replace('\\'', '').replace('\"', '')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceIFrameChecker(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('has_iframe', fn.udf(SourceIFrameChecker._has_iframe, t.BooleanType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _has_iframe(soup):\n",
    "        \"\"\"\n",
    "        iFrame in Body\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('iframe')\n",
    "        return int(0 == len(sources))\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class SourceHyperlinkCounter(Transformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn('has_iframe', fn.udf(SourceHyperlinkCounter._count_hyperlink, t.BooleanType())('bs_obj'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_hyperlink(soup):\n",
    "        \"\"\"\n",
    "        No of hyperlink\n",
    "        :param soup:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sources = soup.findAll('a')\n",
    "        return len([1 for source in sources if source.has_attr('href') and source['href'].lower().startswith('http')])\n",
    "\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class FeatureValueMapper(Transformer):\n",
    "    def __init__(self, column_name, mapping):\n",
    "        self._column_name = column_name\n",
    "        self._mapping = mapping\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.replace(to_replace=self._mapping, subset=[self._column_name])\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class NanToZeroConverter(Transformer):\n",
    "    def __init__(self, columns):\n",
    "        self._columns = columns\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_columns = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "\n",
    "        return df.fillna(0, subset=existing_columns)\n",
    "    \n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class FeaturePicker(Transformer):\n",
    "    def __init__(self, features):\n",
    "        self._features = features\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        existing_columns = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "        \n",
    "        return df.select(*existing_columns)\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class DummySuffixDescritizer(Transformer):\n",
    "    def __init__(self, features):\n",
    "        self._features = features\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        categories = df.select(\"suffix\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        exprs = [\n",
    "            fn.when(F.col(\"suffix\") == category, 1).otherwise(0).alias(category)\n",
    "            for category in categories\n",
    "        ]\n",
    "        dummy_df = df.select(fn.col('id'), fn.('suffix')).select('id', *exprs)\n",
    "\n",
    "        return df.join(dummy_df, 'id', 'inner').drop('suffix')\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class FeatureRemover(Transformer):\n",
    "    def __init__(self, features):\n",
    "        self._features = features\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        removed_features = [col_name for col_name in self._columns if col_name in df.schema.names]\n",
    "        return df.drop(*removed_features)\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "    FeatureRemover([\n",
    "        'is_port_access',\n",
    "    ]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    LastAppearBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "    FeatureRemover([\n",
    "        'is_port_access',\n",
    "    ]),\n",
    "    StringIndexer(\n",
    "                inputCol=\"charset\", outputCol=\"charset\", \n",
    "                handleInvalid=\"error\", stringOrderType=\"frequencyDesc\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class CustomizedStandardizer(Transformer):\n",
    "    def __init__(self, norm='l2'):\n",
    "        self._pipe = Pipeline([\n",
    "            ('standard_scaler', preprocessing.StandardScaler()),\n",
    "\n",
    "        ])\n",
    "        self._columns = None\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        \n",
    "        df_unique = df.agg(*(fn.countDistinct(fn.col(col_name)).alias(col_name) for col_name in df.schema.names))\n",
    "        \n",
    "        \n",
    "        return df\n",
    "\n",
    "    \n",
    "# class CustomizedStandardizer(BaseEstimator, TransformerMixin):\n",
    "#     \"\"\"\n",
    "#     Add Sklearn Build-in Function\n",
    "#     \"\"\"\n",
    "#     def __init__(self, norm='l2'):\n",
    "#         self._pipe = Pipeline([\n",
    "#             ('standard_scaler', preprocessing.StandardScaler()),\n",
    "\n",
    "#         ])\n",
    "#         self._columns = None\n",
    "\n",
    "#     @property\n",
    "#     def columns(self):\n",
    "#         return self._columns\n",
    "\n",
    "#     def fit(self,x,y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self,x,y=None):\n",
    "#         result = x\n",
    "\n",
    "#         df_unique = pd.DataFrame()\n",
    "#         for col_name in result.drop('label', axis=1).columns:\n",
    "#             df_unique[col_name] = [len(result[col_name].unique())]\n",
    "\n",
    "#         df_unique.index = ['unique count']\n",
    "#         df_unique = df_unique.T.squeeze()\n",
    "\n",
    "#         binary_columns = df_unique[df_unique < 3].index.tolist()\n",
    "#         numeric_columns = x.drop([*binary_columns, 'label'], axis=1).select_dtypes(include=np.number).columns.tolist()\n",
    "#         other_columns = x.drop([*binary_columns, *numeric_columns, 'label'], axis=1).columns.tolist()\n",
    "#         label = x.label.tolist()\n",
    "#         label = np.array([label]).T\n",
    "\n",
    "#         result = label\n",
    "#         if len(binary_columns) > 0:\n",
    "#             result = np.append(result, x[binary_columns], axis=1)\n",
    "#         if len(numeric_columns) > 0:\n",
    "#             numeric_result = self._pipe.fit_transform(x[numeric_columns])\n",
    "#             result = np.append(result, numeric_result, axis=1)\n",
    "#         if len(other_columns) > 0:\n",
    "#             result = np.append(result, x[other_columns], axis=1)\n",
    "\n",
    "#         result = pd.DataFrame(result, columns= ['label', *binary_columns, *numeric_columns, *other_columns])\n",
    "#         self._columns = [*binary_columns, *numeric_columns, *other_columns, 'label']\n",
    "\n",
    "# #         result.loc[:, 'label'] = x.label-1970\n",
    "#         return result[self._columns]\n",
    "\n",
    "pipe  = Pipeline(stages=[\n",
    "    LabelBuilder(),\n",
    "    URLParser(),\n",
    "    URLLengthCounter(),\n",
    "    URLDepthCounter(),\n",
    "    HasWWWConverter(),\n",
    "    SubdomainLevelCounter(),\n",
    "    RequestParameterCounter(),\n",
    "    DomainSuffixBuilder(),\n",
    "    IncorrectDomainUrlCleaner(),\n",
    "    ColumnRenamer({'scheme': 'protocol_type'}),\n",
    "    BinaryNAEncoder(['content_type']),\n",
    "    EmptyHTMLFilter(),\n",
    "    SourceCodeByteCounter(),\n",
    "    HTML5Justifier(),\n",
    "    BeautifulSoupParser(),\n",
    "    SourceTitleLengthParser(),\n",
    "    SourceInternalJSLibCounter(),\n",
    "    SourceExternalJSLibCounter(),\n",
    "    SourceCharsetParser(),\n",
    "    SourceIFrameChecker(),\n",
    "    SourceHyperlinkCounter(),\n",
    "    FeatureValueMapper('protocol_type', {\n",
    "        'http': 1,\n",
    "        'https':0,\n",
    "    }),\n",
    "    NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "    ]),\n",
    "    FeaturePicker([\n",
    "\n",
    "        'protocol_type',\n",
    "        'url_depth',\n",
    "        'has_www',\n",
    "        'subdomain_level',\n",
    "        'param_cnt',\n",
    "        'suffix_idx',\n",
    "        'is_port_access',\n",
    "        'code_size',\n",
    "        'title_length',\n",
    "        'internal_js_cnt',\n",
    "        'external_js_cnt',\n",
    "        'charset',\n",
    "        'is_html5',\n",
    "        'has_iframe',\n",
    "        'hyperlink_cnt',\n",
    "        'first_appear',\n",
    "\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation',\n",
    "\n",
    "        'label',\n",
    "    ]),\n",
    "    DummySuffixDescritizer(['int', 'org', 'gov', 'in', 'eu', 'cn', 'kr', 'en']),\n",
    "    FeatureRemover([\n",
    "        'is_port_access',\n",
    "    ]),\n",
    "    StringIndexer(\n",
    "                inputCol=\"charset\", outputCol=\"charset\", \n",
    "                handleInvalid=\"error\", stringOrderType=\"frequencyDesc\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.limit(2000).toPandas().to_json('tmp_spark.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 36)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_json('tmp_spark.json', orient='index')\n",
    "\n",
    "raw_data = spark.createDataFrame(raw_data)\n",
    "\n",
    "shape(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6th Edition - Combine suffix dummy with MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import categorical_encoders\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('label_builder', TobitLabelBuilder()),\n",
    "    ('url_parser', URLParser()),\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('binary_na_encoder', BinaryNAEncoder(['content_type'])),\n",
    "    ('html_parser', html_parser),\n",
    "    ('binary_feature_converter', FeatureValueMapper('protocol_type', {\n",
    "                                        'http': 1,\n",
    "                                        'https':0,\n",
    "                                        })),\n",
    "\n",
    "    ('nan_to_Zero_converter', NanToZeroConverter([\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation'\n",
    "    ])),\n",
    "    \n",
    "    ('feature_picker', FeaturePicker([\n",
    "                                        'protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix_idx',\n",
    "                                        'is_port_access',\n",
    "                                        'code_size',\n",
    "                                        'title_length',\n",
    "                                        'internal_js_cnt',\n",
    "                                        'external_js_cnt',\n",
    "                                        'charset',\n",
    "                                        'is_html5',\n",
    "                                        'has_iframe',\n",
    "                                        'hyperlink_cnt',\n",
    "                                        'first_appear',\n",
    "\n",
    "                                        'total_num_of_paper_citing',\n",
    "                                        'total_num_of_author_citing',\n",
    "                                        'total_num_of_affiliation_citing',\n",
    "                                        'total_num_of_journal_citing',\n",
    "                                        'total_num_of_author_self_citation',\n",
    "                                        'total_num_of_affiliation_self_citation',\n",
    "                                        'total_num_of_journal_self_citation',\n",
    "                                        'avg_year',\n",
    "                                        'min_year',\n",
    "                                        'max_year',\n",
    "                                        'median',\n",
    "                                        'num_of_author',\n",
    "                                        'num_of_author_citing',\n",
    "                                        'num_of_affiliation_citing',\n",
    "                                        'num_of_journal_citing',\n",
    "                                        'avg_hindex',\n",
    "                                        'first_author_hindex',\n",
    "                                        'last_author_hindex',\n",
    "                                        'avg_mid_author_hindex',\n",
    "                                        'paper_unique_affiliation',\n",
    "\n",
    "                                        'label',\n",
    "                                       ])),\n",
    "    ('dummy_suffix_descritizer', DummySuffixDescritizer()),\n",
    "\n",
    "    ('feature_remover', FeatureRemover([\n",
    "                                        'is_port_access',\n",
    "                                       ])),\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['charset'])),\n",
    "    ('standard_scaler', TobitCustomizedStandardizer(norm='l2')),\n",
    "\n",
    "])\n",
    "\n",
    "pipe.fit_transform(DataSource().raw_data).to_csv('untrunc_data_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
