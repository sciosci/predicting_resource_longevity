{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/jjian03/anaconda3/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import base64\n",
    "import pandas as pd\n",
    "from singleton_decorator import singleton\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2/lib/spark2/')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from sklearn.utils import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "\n",
    "\n",
    "seed=77\n",
    "\n",
    "def shape(df):\n",
    "    return (len(df), len(df.columns))\n",
    "\n",
    "\n",
    "@singleton\n",
    "class DataSource:\n",
    "#     def __init__(self, fract=.002, training_rate=.7, seed=77):\n",
    "    def __init__(self, fract=.003, training_rate=.7, seed=77):\n",
    "        self._spark = SparkSession.builder. \\\n",
    "            config('spark.app.name', 'logistic_regression'). \\\n",
    "            config('spark.driver.memory', '20g').\\\n",
    "            config('spark.network.timeout', '600s').\\\n",
    "            config('spark.driver.maxResultSize', '60g').\\\n",
    "            config('spark.executor.memory', '60g').\\\n",
    "            config('spark.kryoserializer.buffer.max', '1536m').\\\n",
    "            config('spark.cores.max', '50').\\\n",
    "            getOrCreate()\n",
    "        self._sc = self._spark.sparkContext\n",
    "        self._spark_sql = SQLContext(self._sc)\n",
    "        print(self._spark.version)\n",
    "        self._label_name = 'label'\n",
    "        self.re_initialize(fract, training_rate, seed)\n",
    "        print('Initialized')\n",
    "\n",
    "    def load_dataset(self, path, name):\n",
    "        return self._spark.read.parquet(path).registerTempTable(name)\n",
    "\n",
    "    @property\n",
    "    def sparkContext(self):\n",
    "        return self._sc\n",
    "\n",
    "    @property\n",
    "    def sparkSQL(self):\n",
    "        return self._spark_sql\n",
    "\n",
    "    def re_initialize(self, fract, training_rate, seed):\n",
    "\n",
    "        self.load_dataset('/user/jjian03/WebResourceQuality.parquet', 'web_resource_quality')\n",
    "        self.load_dataset('/user/jjian03/WebResourceQuality_pmid.parquet', 'web_resource_quality_pmid')\n",
    "        self.load_dataset('/datasets/MAG_20200403/MAG_Azure_Parquet/mag_parquet/Papers.parquet', 'Paper')\n",
    "        self.load_dataset('/user/lliang06/icon/MAG_publication_features.parquet', 'mag')\n",
    "\n",
    "        self._raw_data = self._spark_sql.sql('''\n",
    "            SELECT wr.id\n",
    "                , wr.url\n",
    "                , wr.actual_scrape_url\n",
    "                , wr.first_appear\n",
    "                , wr.first_available_timestamp\n",
    "                , wr.last_available_timestamp\n",
    "                , wr.header\n",
    "                , wr.html_text\n",
    "                , wr.comment\n",
    "                , wr.from_waybackmachine\n",
    "                , wr.http_status_code\n",
    "                , wr.original_check_failure\n",
    "                , wr.original_check_error_log\n",
    "                , wr.terminate_reason\n",
    "                , wr.terminate_reason_error_log\n",
    "                , wr.label\n",
    "\n",
    "                , m.paperId\n",
    "                , m.total_num_of_paper_citing\n",
    "                , m.total_num_of_author_citing\n",
    "                , m.total_num_of_affiliation_citing\n",
    "                , m.total_num_of_journal_citing\n",
    "                , m.total_num_of_author_self_citation\n",
    "                , m.total_num_of_affiliation_self_citation\n",
    "                , m.total_num_of_journal_self_citation\n",
    "                , m.avg_year\n",
    "                , m.min_year\n",
    "                , m.max_year\n",
    "                , m.median\n",
    "                , m.num_of_author\n",
    "                , m.num_of_author_citing\n",
    "                , m.num_of_affiliation_citing\n",
    "                , m.num_of_journal_citing\n",
    "                , m.avg_hindex\n",
    "                , m.first_author_hindex\n",
    "                , m.last_author_hindex\n",
    "                , m.avg_mid_author_hindex\n",
    "                , m.paper_unique_affiliation\n",
    "\n",
    "            FROM web_resource_quality wr\n",
    "            JOIN web_resource_quality_pmid wr_doi ON wr.id = wr_doi.id\n",
    "            JOIN Paper p ON wr_doi.doi = p.doi\n",
    "            JOIN mag m ON p.paperId = m.paperId\n",
    "            WHERE wr.label IS NOT NULL\n",
    "            AND wr.label IN (\"0\", \"1\")\n",
    "            AND wr.first_appear IS NOT NULL\n",
    "        ''') \\\n",
    "        .orderBy(fn.rand()) \\\n",
    "        .sample(False, fract, seed) \\\n",
    "        .toPandas()\n",
    "\n",
    "        print(f'Sample Size - raw_data: {len(self._raw_data)}')\n",
    "\n",
    "        self._train_data, self._test_data = train_test_split(self._raw_data, test_size=1-training_rate, random_state=seed)\n",
    "\n",
    "        self._X_raw = self._raw_data.drop(self._label_name, axis=1)\n",
    "        self._y_raw = self._raw_data[self._label_name]\n",
    "\n",
    "        self._X_train = self._train_data.drop(self._label_name, axis=1)\n",
    "        self._y_train = self._train_data[self._label_name]\n",
    "\n",
    "        self._X_test = self._test_data.drop(self._label_name, axis=1)\n",
    "        self._y_test = self._test_data[self._label_name]\n",
    "\n",
    "    @property\n",
    "    def raw_data(self):\n",
    "        return self._raw_data\n",
    "\n",
    "    @property\n",
    "    def X_raw(self):\n",
    "        return self._X_raw\n",
    "\n",
    "    @property\n",
    "    def y_raw(self):\n",
    "        return self._y_raw\n",
    "\n",
    "    @property\n",
    "    def X_train(self):\n",
    "        return self._X_train\n",
    "\n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self._y_train\n",
    "\n",
    "    @property\n",
    "    def X_test(self):\n",
    "        return self._X_test\n",
    "\n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self._y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.cloudera2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/hzhuang/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41797)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:41797)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-58da91df901c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/singleton_decorator/decorator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"\"\"Returns a single instance of decorated class\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1eb812fc4e68>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fract, training_rate, seed)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1eb812fc4e68>\u001b[0m in \u001b[0;36mre_initialize\u001b[0;34m(self, fract, training_rate, seed)\u001b[0m\n\u001b[1;32m    111\u001b[0m         ''') \\\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcolumns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mschema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_datatype_json_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 raise Exception(\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:41797)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "raw_data = DataSource().raw_data\n",
    "\n",
    "raw_data.info()\n",
    "\n",
    "print(f'raw_data: {shape(raw_data)}')\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "X_raw = DataSource().X_raw\n",
    "y_raw = DataSource().y_raw\n",
    "\n",
    "X_train = DataSource().X_train\n",
    "y_train = DataSource().y_train\n",
    "\n",
    "X_test = DataSource().X_test\n",
    "y_test = DataSource().y_test\n",
    "\n",
    "print('Shape of the dataframe:')\n",
    "print(f'X_raw: {shape(X_raw)}')\n",
    "print(f'y_raw: {len(y_raw)}')\n",
    "print()\n",
    "print(f'X_train: {shape(X_train)}')\n",
    "print(f'y_train: {len(y_train)}')\n",
    "print()\n",
    "print(f'X_test: {shape(X_test)}')\n",
    "print(f'y_test: {len(y_test)}')\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - First Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = X_train\n",
    "result.loc[:,'label'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features in URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length of the url hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "class URLLengthCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:,'url_length'] = result['url'].apply(self._get_length)\n",
    "        return result\n",
    "\n",
    "    def _get_length(self, url):\n",
    "        return len(url)\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['url', 'url_length']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "class URLParser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._lambdas = dict()\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:,'url_parse_obj'] = result['url'].apply(urlparse)\n",
    "\n",
    "        result.loc[:,'scheme'] = result.url_parse_obj.apply(lambda x: x.scheme)\n",
    "        result.loc[:,'netloc'] = result.url_parse_obj.apply(lambda x: x.netloc)\n",
    "        result.loc[:,'path'] = result.url_parse_obj.apply(lambda x: x.path)\n",
    "        result.loc[:,'params'] = result.url_parse_obj.apply(lambda x: x.query).apply(lambda x: None if '' == x.strip() else x)\n",
    "\n",
    "        result = result.drop(['url_parse_obj'], axis=1)\n",
    "        return result\n",
    "\n",
    "    def register_new_column(self, col_name, lbd):\n",
    "        self._lambdas[col_name] = lbd\n",
    "        return self\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('url_parser', URLParser()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['path', 'scheme', 'netloc', 'path', 'params']][result.params.isna().apply(lambda x: not x)].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth of the url hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLDepthCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:,'url_depth'] = result['path'].apply(self._get_depth)\n",
    "        return result\n",
    "\n",
    "    def _get_depth(self, path):\n",
    "        last_idx = path.rindex('/')\n",
    "        if last_idx + 1 < len(path):\n",
    "            last_idx = len(path)\n",
    "        return path[:last_idx].count('/')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['path', 'url_depth']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Has WWW subdomain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HasWWWConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:, 'has_www'] = result['netloc'].apply(self._has_www)\n",
    "        return result\n",
    "\n",
    "    def _has_www(self, domain):\n",
    "        return int(domain.startswith('www.'))\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['netloc', 'has_www']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level of the Subdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubdomainLevelCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:, 'subdomain_level'] = result['netloc'].apply(self._get_level)\n",
    "        return result\n",
    "\n",
    "    def _get_level(self, domain):\n",
    "        return domain.count('.')\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['netloc', 'subdomain_level']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of HTTP-Get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RequestParameterCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result['params'] = result['params'].replace(np.nan, '', regex=True)\n",
    "        result.loc[:, 'param_cnt'] = result['params'].apply(self._count_param)\n",
    "        return result\n",
    "\n",
    "    def _count_param(self, params):\n",
    "        if params is '':\n",
    "            return 0\n",
    "        return params.count('&') + 1\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['params', 'param_cnt']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/jjian03/anaconda3/bin/pip install feature_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import categorical_encoders\n",
    "\n",
    "\n",
    "class DomainSuffixBuilder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._suffix_dict = None\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        new_features = self.build_suffix_port_feature(x)\n",
    "        new_features = new_features.dropna()\n",
    "\n",
    "        encoder = categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "            encoding_method='frequency',\n",
    "            variables=['suffix'])\n",
    "        encoder.fit(new_features)\n",
    "        self._suffix_dict = encoder.encoder_dict_['suffix']\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        new_features = self.build_suffix_port_feature(x)\n",
    "        for col_name in new_features.columns:\n",
    "            result.loc[:, col_name] = new_features[col_name]\n",
    "        result.loc[:, 'suffix'] = result.suffix.apply(lambda v: self._suffix_dict[v] if v in self._suffix_dict else 0)\n",
    "\n",
    "        result = result.dropna(subset=['is_port_access', 'suffix', 'suffix_idx'])\n",
    "        return result\n",
    "\n",
    "    def build_suffix_port_feature(self,x):\n",
    "        result = x\n",
    "        # Remove incorrect urls\n",
    "#         result = result[result['netloc'].apply(lambda val: '.' in val)]\n",
    "        # Build features\n",
    "        suffix = result.netloc.apply(DomainSuffixBuilder._get_url_suffix)\n",
    "        is_port_access = suffix.apply(DomainSuffixBuilder._is_port_access)\n",
    "        suffix_idx = suffix.apply(DomainSuffixBuilder._clean_url_suffix)\n",
    "        \n",
    "        return pd.DataFrame({'suffix': suffix, 'suffix_idx': suffix_idx, 'is_port_access': is_port_access, })\n",
    "\n",
    "    @property\n",
    "    def suffix_dict(self):\n",
    "        return self._suffix_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_url_suffix(url):\n",
    "        if not '.' in url:\n",
    "            return None\n",
    "        last_idx = url.rindex('.')\n",
    "        return url[last_idx + 1:]\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_url_suffix(url):\n",
    "        if None is url:\n",
    "            return None\n",
    "        return url.split(':')[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_port_access(suffix):\n",
    "        if None is suffix:\n",
    "            return None\n",
    "        return int(len([token for token in suffix.split(':') if token.strip() != '']) > 1)\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "])\n",
    "\n",
    "result = pipe.fit_transform(result)\n",
    "\n",
    "\n",
    "display(result[['netloc', 'is_port_access', 'suffix', 'suffix_idx']].head(5))\n",
    "display(pd.Series(list(pipe.steps[-1][1].suffix_dict.values()), index = pipe.steps[-1][1].suffix_dict.keys()) \\\n",
    "        .head().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.suffix_idx.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the Incorrect Domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TLD ranges from 2 to 63\n",
    "\n",
    "Ref: https://en.wikipedia.org/wiki/Domain_Name_System#cite_ref-rfc1034_1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class IncorrectDomainUrlCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # TLD ranges from 2 to 63\n",
    "        self._regex = re.compile(r'^[a-zA-Z]{2,63}$', re.I)\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:, 'is_correct'] = result.suffix_idx.apply(self._is_correct)\n",
    "        result = result[result.is_correct]\n",
    "        result = result.drop('is_correct', axis=1)\n",
    "        return result\n",
    "\n",
    "    def _is_correct(self, domain_suffix):\n",
    "        return True if self._regex.match(domain_suffix) else False\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "print(f'Before changes: {len(X_train)}')\n",
    "print(f'After changes: {len(result)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protocol Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import categorical_encoders\n",
    "\n",
    "\n",
    "class ColumnRenamer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mapping):\n",
    "        self._mapping = mapping\n",
    "\n",
    "    @property\n",
    "    def mapping(self):\n",
    "        return self._mapping\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        self._mapping = {key: value for key, value in self._mapping.items() if key in result.columns}\n",
    "        result = result.rename(columns=self._mapping)\n",
    "        return result\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['url', 'protocol_type']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class Formatter:\n",
    "    @staticmethod\n",
    "    def get_timestamp(format=\"%Y%m%d_%H%M%S\"):\n",
    "        return str((datetime.now().strftime(format)))\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower_case_dict(pair):\n",
    "        if None is pair:\n",
    "            return dict()\n",
    "        return dict((k.lower(), v.lower()) for k, v in pair.items())\n",
    "\n",
    "\n",
    "class ContentTypeExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:,'content_type'] = result.header \\\n",
    "            .apply(eval) \\\n",
    "            .apply(Formatter.to_lower_case_dict) \\\n",
    "            .apply(lambda x: x['content-type'] if 'content-type' in x else None)\n",
    "\n",
    "        return result\n",
    "\n",
    "    \n",
    "pipe = Pipeline([\n",
    "    ('content_type_extractor', ContentTypeExtractor()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['content_type']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNAEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self._columns = columns\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        self._columns = [col_name for col_name in self._columns if col_name in result.columns]\n",
    "        for col_name in self._columns:\n",
    "            result.loc[:,f'has_{col_name}'] = result[col_name] \\\n",
    "                .apply(lambda x: x not in [np.nan, None]) \\\n",
    "                .map({True: 1, False: 0})\n",
    "\n",
    "        return result\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('binary_na_encoder', BinaryNAEncoder(['content_type'])),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "display(result[['has_content_type']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_count(df):\n",
    "    df_unique = pd.DataFrame()\n",
    "    for col_name in df.columns:\n",
    "        df_unique[col_name] = [len(df[col_name].unique())]\n",
    "\n",
    "    df_unique['total'] = [len(df)]\n",
    "    df_unique.index = ['unique count']\n",
    "    return df_unique.T.iloc[:,0]\n",
    "\n",
    "def print_na_count(df):\n",
    "    df_na = pd.DataFrame()\n",
    "    for col_name in df.columns:\n",
    "        df_na[col_name] = [df[col_name].isna().sum()]\n",
    "\n",
    "    df_na['total'] = [len(df)]\n",
    "    df_na.index = ['na count']\n",
    "    return df_na.T.iloc[:,0]\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    'unique count': print_unique_count(result),\n",
    "    'na count': print_na_count(result)\n",
    "}, index=result.columns))\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect empty html records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_html_records = result[result.html_text.isna()]\n",
    "pdf_records = no_html_records.content_type.str.startswith('application/pdf').sum()\n",
    "print(f'total empty html records: {len(no_html_records)}, pdf out of those records: {pdf_records}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionable_records = result[result.html_text.isna() & result.content_type.str.startswith('application/pdf').apply(lambda x: not x)]\n",
    "questionable_records.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionable_records.content_type.str.startswith('application/json').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/jjian03/anaconda3/bin/pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('url_length_counter', URLLengthCounter()),\n",
    "#     ('url_depth_counter', URLDepthCounter()),\n",
    "#     ('has_www_converter', HasWWWConverter()),\n",
    "#     ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "#     ('request_parameter_counter', RequestParameterCounter()),\n",
    "#     ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "#     ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "#     ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "# ])\n",
    "\n",
    "# result = pipe.transform(X_train)\n",
    "\n",
    "non_binary_result = result[['protocol_type', 'url_length', 'url_depth', 'subdomain_level', 'param_cnt', 'suffix_idx']]\n",
    "\n",
    "def plot_distribution(data, title, height=1200, width=800):\n",
    "    fig = make_subplots(rows=len(data.columns), cols=1,\n",
    "                    subplot_titles=data.columns)\n",
    "\n",
    "    for idx, col_name in enumerate(data.columns):\n",
    "        fig.add_trace(go.Histogram(x=data[col_name], name=col_name), row=idx + 1, col=1)\n",
    "\n",
    "\n",
    "    fig.update_layout(height=height, width=width, title_text=title)\n",
    "    return fig\n",
    "\n",
    "plot_distribution(non_binary_result, \"Non Binary Features Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "binary_result = result[['label', 'has_www', 'is_port_access', 'has_content_type']]\n",
    "\n",
    "\n",
    "plot_distribution(binary_result, \"Binary Features Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the non-binary feature are right skewed, it is necessary to apply the standard scaler at the later process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age of the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import datetime\n",
    "from dateutil import relativedelta\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "\n",
    "class ChronologyBuilder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._scraped_dt = datetime.datetime.strptime('20200525132015', \"%Y%m%d%H%M%S\")\n",
    "#         self._scraped_dt = datetime.datetime.now()\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:, 'timestamp_coef'] = result.last_available_timestamp \\\n",
    "            .apply(self._convert_timestamp_to_coef) \\\n",
    "            .fillna(self._extract_year(result.id.apply(ObjectId))) \\\n",
    "            .astype(int)\n",
    "        return result\n",
    "\n",
    "    def _extract_year(self, ids):\n",
    "        return ids.apply(lambda x: x.generation_time.year)\n",
    "\n",
    "    def _convert_timestamp_to_coef(self, ts):\n",
    "        if None is ts or np.nan is ts or math.isnan(ts):\n",
    "            return ts\n",
    "        ts_str = str(ts).strip()\n",
    "        if '' == ts_str:\n",
    "            return ts\n",
    "\n",
    "        ts_str = str(int(float(ts_str)))\n",
    "        ts_obj = datetime.datetime.strptime(ts_str, \"%Y%m%d%H%M%S\")\n",
    "        return ts_obj.year\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('chronology_builder', ChronologyBuilder()),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "result.timestamp_coef.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self._removed_features = None\n",
    "        self._features = features\n",
    "\n",
    "    @property\n",
    "    def removed_features(self):\n",
    "        return self._removed_features\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        self._removed_features = [col_name for col_name in self._features if col_name in result.columns]\n",
    "        result = result.drop(self._removed_features, axis=1)\n",
    "        return result\n",
    "\n",
    "\n",
    "class FeaturePicker(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self._picked_features = None\n",
    "        self._features = features\n",
    "\n",
    "    @property\n",
    "    def picked_features(self):\n",
    "        return self._picked_features\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        self._picked_features = [col_name for col_name in self._features if col_name in result.columns]\n",
    "        result = result[self._picked_features]\n",
    "        return result\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_picker', FeaturePicker(['protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix',\n",
    "                                        'timestamp_coef',\n",
    "                                        'is_port_access',\n",
    "                                        'label',\n",
    "                                       ])),\n",
    "])\n",
    "\n",
    "result = pipe.transform(result)\n",
    "\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous Clean Up\n",
    "\n",
    "- Standardize variance\n",
    "- Convert Categorical Feature into Frequency Based Numberical Index \n",
    "- Remove low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from sklearn import feature_selection\n",
    "\n",
    "\n",
    "class LowVarianceRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold):\n",
    "        self._p = threshold\n",
    "        self._bi_vt = feature_selection.VarianceThreshold(threshold=threshold*(1-threshold))\n",
    "        self._regular_vt = feature_selection.VarianceThreshold(threshold=threshold)\n",
    "        self._dropped_columns = list()\n",
    "\n",
    "    @property\n",
    "    def threshold(self):\n",
    "        return self._threshold\n",
    "\n",
    "    @property\n",
    "    def dropped_columns(self):\n",
    "        return self._dropped_columns\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "\n",
    "        df_unique = pd.DataFrame()\n",
    "        for col_name in result.columns:\n",
    "            if 'label' != col_name:\n",
    "                df_unique[col_name] = [len(result[col_name].unique())]\n",
    "\n",
    "        df_unique.index = ['unique count']\n",
    "        df_unique = df_unique.T.squeeze()\n",
    "\n",
    "        bi_columns = df_unique[df_unique == 2].index.tolist()\n",
    "        regular_columns = df_unique[df_unique != 2].index.tolist()\n",
    "\n",
    "        if len(bi_columns) >0:\n",
    "            self._bi_vt.fit(result[bi_columns])\n",
    "            bi_mask = self._bi_vt.variances_ < self._p * (1 - self._p)            \n",
    "            self._dropped_columns = self._dropped_columns + list(compress(bi_columns, bi_mask))\n",
    "        if len(regular_columns) >0 :\n",
    "            self._regular_vt.fit(result[regular_columns])\n",
    "            regular_mask = self._regular_vt.variances_ < self._p\n",
    "            self._dropped_columns = self._dropped_columns + list(compress(regular_columns, regular_mask))\n",
    "\n",
    "        if len(self._dropped_columns) > 0:\n",
    "            remover = FeatureRemover(self._dropped_columns)\n",
    "            result = remover.transform(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['protocol_type'])),\n",
    "    ('low_variance_remover', LowVarianceRemover(0.01))\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "result = pipe.fit_transform(result)\n",
    "\n",
    "\n",
    "print(f'Before transform: {X_train.columns}\\n')\n",
    "print(f'After transform: {result.columns}\\n')\n",
    "print(f'Dropped columns: {pipe.steps[-1][1].dropped_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The port indicator is wiped out, but I believe this could be a reason to explain the availability of the url resource, so I will separately build a subset to analyze that part later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import multiprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "cpu_cnt = multiprocessing.cpu_count()\n",
    "allocated_cpu = cpu_cnt\n",
    "print(f\"Allocated {allocated_cpu} CPUs\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "class AnalysisEngineBuilder:\n",
    "\n",
    "    def __init_(self):\n",
    "        self._X_train = None\n",
    "        self._y_train = None\n",
    "        self._X_test = None\n",
    "        self._y_test = None\n",
    "        self._param_grid = None\n",
    "        self._engine = None\n",
    "    def set_X_train(self, X_train):\n",
    "        self._X_train = X_train\n",
    "        return self\n",
    "    def set_y_train(self, y_train):\n",
    "        self._y_train = y_train\n",
    "        return self\n",
    "    def set_X_test(self, X_test):\n",
    "        self._X_test = X_test\n",
    "        return self\n",
    "    def set_y_test(self, y_test):\n",
    "        self._y_test = y_test\n",
    "        return self\n",
    "    def set_param_grid(self, param_grid):\n",
    "        self._param_grid = param_grid\n",
    "        return self\n",
    "    def set_engine(self, engine):\n",
    "        self._engine = engine\n",
    "        return self\n",
    "    def build(self):\n",
    "        return AnalysisEngineBuilder._AnalysisEngine(self._X_train, self._y_train, self._X_test, self._y_test, self._param_grid, self._engine)\n",
    "\n",
    "    class _AnalysisEngine:\n",
    "        def __init__(self, X_train, y_train, X_test, y_test, param_grid, engine):\n",
    "            self._X_train = X_train\n",
    "            self._y_train = y_train\n",
    "            self._X_test = X_test\n",
    "            self._y_test = y_test\n",
    "            self._param_grid = param_grid\n",
    "            self._engine = engine\n",
    "            self._grid = GridSearchCV(self._engine, self._param_grid, cv=10, scoring='roc_auc')\n",
    "            self._pred = None\n",
    "            self._pred_prob = None\n",
    "            self._accuracy = None\n",
    "            self._roc = None\n",
    "            self._tpr = None\n",
    "            self._fpr = None\n",
    "        @property\n",
    "        def grid_search_result(self):\n",
    "            return pd.DataFrame(self._grid.cv_results_)\n",
    "        @property\n",
    "        def accuracy(self):\n",
    "            return self._accuracy\n",
    "        @property\n",
    "        def roc(self):\n",
    "            return self._roc\n",
    "        @property\n",
    "        def tpr(self):\n",
    "            return self._tpr\n",
    "        @property\n",
    "        def fpr(self):\n",
    "            return self._fpr\n",
    "        @property\n",
    "        def threshold(self):\n",
    "            return self._threshold\n",
    "        def analyze(self):\n",
    "            self._grid.fit(self._X_train, self._y_train)\n",
    "            self._pred = self._grid.predict(self._X_test)\n",
    "            self._fpr, self._tpr, self._threshold = roc_curve(self._y_test, self._pred)\n",
    "            try:\n",
    "                self._pred_prob = self._grid.predict_proba(self._X_test)\n",
    "                self._pred_prob = pd.DataFrame(self._pred_prob)[1]\n",
    "                self._fpr, self._tpr, self._threshold = roc_curve(self._y_test, self._pred_prob)\n",
    "            except AttributeError as ae:\n",
    "                self._pred_prob = self._pred\n",
    "            self._accuracy = accuracy_score(self._y_test, self._pred)\n",
    "            self._roc = roc_auc_score(self._y_test, self._pred_prob)\n",
    "\n",
    "            return self._grid\n",
    "\n",
    "        def show_performance(self):\n",
    "            print(f\"ROC/AUC: {round(self._roc*100, 2)}%\")\n",
    "            print()\n",
    "            print(classification_report(self._y_test, self._pred, target_names=[\"Valid Url\",\"Invalid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "\n",
    "class Visualizer():\n",
    "    @staticmethod\n",
    "    def group_plot_roc_curve(title, data_group):\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(5, 5), dpi=80)\n",
    "\n",
    "        x = [0.0, 1.0]\n",
    "        plt.plot(x, x, linestyle='dashed', color='red', linewidth=2, label='Naive prediction (Random guess)')\n",
    "        for idx, group in enumerate(data_group):\n",
    "            fpr = group[0]\n",
    "            tpr = group[1]\n",
    "            label = group[2]\n",
    "            linestyle= 'solid'\n",
    "            if idx % 2 == 1:\n",
    "                linestyle= 'dashed'\n",
    "            plt.plot(fpr, tpr, linestyle=linestyle, linewidth=2, label=label)\n",
    "\n",
    "        plt.xlim(0.0, 1.0)\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.xlabel(\"FPR\", fontsize=14)\n",
    "        plt.ylabel(\"TPR\", fontsize=14)\n",
    "\n",
    "        plt.legend(fontsize=10, loc='lower right')\n",
    "\n",
    "        plt.title(title, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show()\n",
    "        return plt\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_performance(data,\n",
    "                            legend_type_name,\n",
    "                            x_axis_name,\n",
    "                            upper_y_label,\n",
    "                            lower_y_label,\n",
    "                            title):\n",
    "        plt.clf()\n",
    "        f, ax = plt.subplots(2, 1, figsize=(15,8))\n",
    "        legends = data[legend_type_name].unique()\n",
    "        for idx, legend in enumerate(legends):\n",
    "            _data = data[data[legend_type_name]==legend]\n",
    "            ax[0].plot(_data[x_axis_name], _data[upper_y_label], linewidth=2, label=f'{legend_type_name}: {legend}')\n",
    "            ax[0].set_xlabel(x_axis_name, fontsize=15)\n",
    "            ax[0].set_ylabel(upper_y_label.upper(), fontsize=15)\n",
    "            ax[0].legend(fontsize=10, loc='upper right')\n",
    "\n",
    "            ax[1].plot(_data[x_axis_name], _data[lower_y_label], linewidth=2, label=f'{legend_type_name}: {legend}')\n",
    "            ax[1].set_xlabel(x_axis_name, fontsize=15)\n",
    "            ax[1].set_ylabel(lower_y_label.upper(), fontsize=15)\n",
    "            ax[1].legend(fontsize=10, loc='lower right')\n",
    "\n",
    "        ax[0].set_title(f\"Performance Evaluation of {title}\", fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "        return plt\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_feature_importance(reg_coef, col_names, title):\n",
    "        reg_coef = pd.Series(reg_coef, index=col_names)\n",
    "        reg_coef = reg_coef.sort_values()\n",
    "        matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "        reg_coef.plot(kind=\"barh\",)\n",
    "        plt.title(title, fontsize=15)\n",
    "\n",
    "        return plt\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_importance_trending(X_train, feature_importance_matrix, title, offset=3):\n",
    "        feature_importance = feature_importance_matrix.groupby('C').agg(['mean'])[[*X_train.columns]]\n",
    "        feature_importance.columns = X_train.columns.tolist()\n",
    "        feature_importance['C'] = feature_importance.index\n",
    "        \n",
    "        column_names = X_train.columns\n",
    "        lbds = feature_importance['C'].tolist()\n",
    "        coef_matrix = feature_importance[X_train.columns]\n",
    "        x_lab = 'Lambda'\n",
    "        y_lab = 'Weight'\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, col_name in enumerate(column_names):\n",
    "            plt.plot(lbds, coef_matrix.iloc[:,idx], 'o-', linewidth=2, label=col_name)\n",
    "            c = coef_matrix.iloc[0,idx]\n",
    "            plt.annotate(col_name, (lbds[offset], coef_matrix.iloc[offset,idx]))\n",
    "\n",
    "        plt.title(title, fontSize=25)\n",
    "        plt.xlabel(x_lab)\n",
    "        plt.ylabel(y_lab)\n",
    "\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "from sklearn.metrics import hinge_loss\n",
    "\n",
    "\n",
    "def loss_accuracy_analyze_job_builder(X_train, y_train, X_test, y_test, model_func, param):\n",
    "    def _analyze_param_combination():\n",
    "        engine = AnalysisEngineBuilder() \\\n",
    "                    .set_X_train(X_train) \\\n",
    "                    .set_y_train(y_train) \\\n",
    "                    .set_X_test(X_test) \\\n",
    "                    .set_y_test(y_test) \\\n",
    "                    .set_param_grid(param) \\\n",
    "                    .set_engine(model_func) \\\n",
    "                    .build()\n",
    "        model = engine.analyze()\n",
    "        \n",
    "        # Performance scores\n",
    "        proba = pd.DataFrame(model.predict_proba(X_test))[1]\n",
    "        loss = hinge_loss(y_test, proba)\n",
    "        auc = roc_auc_score(y_test, proba)\n",
    "        \n",
    "        coef = pd.Series(model.best_estimator_.coef_[0], index=X_test.columns).to_dict()\n",
    "        _param = param\n",
    "        for key, value in param.items():\n",
    "            _param[key] = value[0]\n",
    "        return {\n",
    "            'accuracy': engine.accuracy * 100,\n",
    "            'loss': loss,\n",
    "            'auc': auc,\n",
    "            **coef,\n",
    "            **_param\n",
    "        }\n",
    "    return _analyze_param_combination\n",
    "\n",
    "# Refactor into the analyzer later on\n",
    "def calculate_grid_performance(X_train, y_train, X_test, y_test, params, model):\n",
    "    # build combination list\n",
    "    combination_list = pd.DataFrame({'dummy': [1]})\n",
    "    for key, values in params.items():\n",
    "        combination_list = pd.merge(combination_list, pd.DataFrame({key: values, 'dummy': [1] * len(values)}))\n",
    "    combination_list.drop('dummy',axis=1, inplace=True)\n",
    "\n",
    "    # Train and extract scores\n",
    "    futures = list()\n",
    "    results = list()\n",
    "    # Execute models in threads\n",
    "    with ThreadPoolExecutor(max_workers=allocated_cpu) as executor:\n",
    "        for combination in combination_list.to_dict('records'):\n",
    "            combination = {key:[value] for key, value in combination.items()}\n",
    "            future_model = executor.submit(loss_accuracy_analyze_job_builder(X_train, y_train, X_test, y_test, model, combination))\n",
    "            futures.append(future_model)\n",
    "        return pd.DataFrame.from_dict([future.result() for future in futures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_parser', URLParser()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('content_type_extractor', ContentTypeExtractor()),\n",
    "    ('binary_na_encoder', BinaryNAEncoder(['content_type'])),\n",
    "    ('chronology_builder', ChronologyBuilder()),\n",
    "    ('feature_picker', FeaturePicker(['protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix',\n",
    "                                        'timestamp_coef',\n",
    "                                        'is_port_access',\n",
    "#                                         'has_content_type',\n",
    "                                        'label'\n",
    "                                       ])),\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['protocol_type'])),\n",
    "    # Low Variance Filter works incorrectly.\n",
    "    ('low_variance_remover', LowVarianceRemover(.005)), # Decreased to .005\n",
    "    ('feature_remover', FeatureRemover([\n",
    "                                        'param_cnt',\n",
    "                                        'is_port_access',\n",
    "                                       ])),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "train = DataSource().X_train\n",
    "train.loc[:,'label'] = DataSource().y_train\n",
    "X_train = pipe.fit_transform(train)\n",
    "y_train = X_train.iloc[:,-1]\n",
    "# X_train = pd.DataFrame(X_train, columns= pipe.steps[-1][1].columns)\n",
    "X_train = X_train.drop('label', axis=1)\n",
    "print(X_train.columns)\n",
    "\n",
    "\n",
    "test = DataSource().X_test\n",
    "test.loc[:,'label'] = DataSource().y_test\n",
    "X_test = pipe.fit_transform(test)\n",
    "y_test = X_test.iloc[:,-1]\n",
    "# X_test = pd.DataFrame(X_test, columns= pipe.steps[-1][1].columns)\n",
    "X_test = X_test.drop('label', axis=1)\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-3, -2, 20)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                       penalty='elasticnet',\n",
    "                       solver='saga',\n",
    "                       multi_class='ovr',\n",
    "                       warm_start=False,\n",
    "                       n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "\n",
    "# Start to train model\n",
    "engine_lr = AnalysisEngineBuilder() \\\n",
    "    .set_X_train(X_train) \\\n",
    "    .set_y_train(y_train) \\\n",
    "    .set_X_test(X_test) \\\n",
    "    .set_y_test(y_test) \\\n",
    "    .set_param_grid(param_lr) \\\n",
    "    .set_engine(lr) \\\n",
    "    .build()\n",
    "\n",
    "model_lr = engine_lr.analyze()\n",
    "engine_lr.show_performance()\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "Visualizer.group_plot_roc_curve('ROC Curve of Logistic Regression', [\n",
    "    (engine_lr.fpr, engine_lr.tpr, 'Logistic Regression')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-3, -2, 20)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                        penalty='elasticnet',\n",
    "                        solver='saga',\n",
    "                        multi_class='ovr',\n",
    "                        warm_start=False,\n",
    "                        n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "loss_accuracy_matrix = calculate_grid_performance(X_train, y_train, X_test, y_test, param_lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizer.plot_performance(data=loss_accuracy_matrix,\n",
    "                    legend_type_name='l1_ratio',\n",
    "                    x_axis_name='C',\n",
    "                    upper_y_label='loss',\n",
    "                    lower_y_label='auc',\n",
    "                    title='Loss& Accuracy - Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizer.plot_importance_trending(X_train, loss_accuracy_matrix, 'Weight change on each feature', offset=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try encode the suffix index label with logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogarithmTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self._columns = columns\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:, self._columns] = (result[self._columns]+0.00000000001).applymap(math.log)\n",
    "\n",
    "        return result\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('binary_na_encoder', BinaryNAEncoder(['content_type'])),\n",
    "    ('chronology_builder', ChronologyBuilder()),\n",
    "    ('feature_picker', FeaturePicker(['protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix',\n",
    "                                        'timestamp_coef',\n",
    "                                        'is_port_access',\n",
    "#                                         'has_content_type',\n",
    "                                        'label'\n",
    "                                       ])),\n",
    "    ('logarithm_transformer', LogarithmTransformer(['suffix'])),\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['protocol_type'])),\n",
    "    # Low Variance Filter works incorrectly.\n",
    "#     ('low_variance_remover', LowVarianceRemover(.005)), # Decreased to .005\n",
    "    ('feature_remover', FeatureRemover([\n",
    "                                        'param_cnt',\n",
    "                                        'is_port_access',\n",
    "                                       ])),\n",
    "\n",
    "])\n",
    "\n",
    "train = DataSource().X_train\n",
    "train.loc[:,'label'] = DataSource().y_train\n",
    "X_train = pipe.fit_transform(train)\n",
    "y_train = X_train.iloc[:,-1]\n",
    "# X_train = pd.DataFrame(X_train, columns= pipe.steps[-1][1].columns)\n",
    "X_train = X_train.drop('label', axis=1)\n",
    "print(X_train.columns)\n",
    "\n",
    "\n",
    "test = DataSource().X_test\n",
    "test.loc[:,'label'] = DataSource().y_test\n",
    "X_test = pipe.fit_transform(test)\n",
    "y_test = X_test.iloc[:,-1]\n",
    "# X_test = pd.DataFrame(X_test, columns= pipe.steps[-1][1].columns)\n",
    "X_test = X_test.drop('label', axis=1)\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-3, -2, 20)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                       penalty='elasticnet',\n",
    "                       solver='saga',\n",
    "                       multi_class='ovr',\n",
    "                       warm_start=False,\n",
    "                       n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "\n",
    "# Start to train model\n",
    "engine_lr = AnalysisEngineBuilder() \\\n",
    "    .set_X_train(X_train) \\\n",
    "    .set_y_train(y_train) \\\n",
    "    .set_X_test(X_test) \\\n",
    "    .set_y_test(y_test) \\\n",
    "    .set_param_grid(param_lr) \\\n",
    "    .set_engine(lr) \\\n",
    "    .build()\n",
    "\n",
    "model_lr = engine_lr.analyze()\n",
    "engine_lr.show_performance()\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "Visualizer.group_plot_roc_curve('ROC Curve of Logistic Regression', [\n",
    "    (engine_lr.fpr, engine_lr.tpr, 'Logistic Regression')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-3, -2, 20)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                        penalty='elasticnet',\n",
    "                        solver='saga',\n",
    "                        multi_class='ovr',\n",
    "                        warm_start=False,\n",
    "                        n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "loss_accuracy_matrix = calculate_grid_performance(X_train, y_train, X_test, y_test, param_lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizer.plot_performance(data=loss_accuracy_matrix,\n",
    "                    legend_type_name='l1_ratio',\n",
    "                    x_axis_name='C',\n",
    "                    upper_y_label='loss',\n",
    "                    lower_y_label='auc',\n",
    "                    title='Loss& Accuracy - Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizer.plot_importance_trending(X_train, loss_accuracy_matrix, 'Weight change on each feature', offset=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Tong's methd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySuffixDescritizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        dummies = pd.get_dummies(result.suffix_idx)\n",
    "        dummies = FeaturePicker(['int', 'org', 'gov', 'in', 'eu', 'cn']).fit_transform(dummies)\n",
    "        result = result.drop('suffix_idx', axis = 1).join(dummies, how='inner')\n",
    "\n",
    "        return result\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('binary_na_encoder', BinaryNAEncoder(['content_type'])),\n",
    "    ('chronology_builder', ChronologyBuilder()),\n",
    "    ('feature_picker', FeaturePicker(['protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix_idx',\n",
    "                                        'timestamp_coef',\n",
    "                                        'is_port_access',\n",
    "#                                         'has_content_type',\n",
    "                                        'label'\n",
    "                                       ])),\n",
    "    ('dummy_suffix_descritizer', DummySuffixDescritizer()),\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['protocol_type'])),\n",
    "    # Low Variance Filter works incorrectly.\n",
    "#     ('low_variance_remover', LowVarianceRemover(.005)), # Decreased to .005\n",
    "    ('feature_remover', FeatureRemover([\n",
    "                                        'param_cnt',\n",
    "                                        'is_port_access',\n",
    "                                       ])),\n",
    "\n",
    "])\n",
    "\n",
    "train = DataSource().X_train\n",
    "train.loc[:,'label'] = DataSource().y_train\n",
    "X_train = pipe.fit_transform(train)\n",
    "y_train = X_train.iloc[:,-1]\n",
    "# X_train = pd.DataFrame(X_train, columns= pipe.steps[-1][1].columns)\n",
    "X_train = X_train.drop('label', axis=1)\n",
    "print(X_train.columns)\n",
    "\n",
    "\n",
    "test = DataSource().X_test\n",
    "test.loc[:,'label'] = DataSource().y_test\n",
    "X_test = pipe.fit_transform(test)\n",
    "y_test = X_test.iloc[:,-1]\n",
    "# X_test = pd.DataFrame(X_test, columns= pipe.steps[-1][1].columns)\n",
    "X_test = X_test.drop('label', axis=1)\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-3, -2, 20)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                       penalty='elasticnet',\n",
    "                       solver='saga',\n",
    "                       multi_class='ovr',\n",
    "                       warm_start=False,\n",
    "                       n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "\n",
    "# Start to train model\n",
    "engine_lr = AnalysisEngineBuilder() \\\n",
    "    .set_X_train(X_train) \\\n",
    "    .set_y_train(y_train) \\\n",
    "    .set_X_test(X_test) \\\n",
    "    .set_y_test(y_test) \\\n",
    "    .set_param_grid(param_lr) \\\n",
    "    .set_engine(lr) \\\n",
    "    .build()\n",
    "\n",
    "model_lr = engine_lr.analyze()\n",
    "engine_lr.show_performance()\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "Visualizer.group_plot_roc_curve('ROC Curve of Logistic Regression', [\n",
    "    (engine_lr.fpr, engine_lr.tpr, 'Logistic Regression')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-3, -2, 20)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                        penalty='elasticnet',\n",
    "                        solver='saga',\n",
    "                        multi_class='ovr',\n",
    "                        warm_start=False,\n",
    "                        n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "loss_accuracy_matrix = calculate_grid_performance(X_train, y_train, X_test, y_test, param_lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizer.plot_performance(data=loss_accuracy_matrix,\n",
    "                    legend_type_name='l1_ratio',\n",
    "                    x_axis_name='C',\n",
    "                    upper_y_label='loss',\n",
    "                    lower_y_label='auc',\n",
    "                    title='Loss& Accuracy - Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizer.plot_importance_trending(X_train, loss_accuracy_matrix, 'Weight change on each feature', offset=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Second Round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features in source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Restore the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First round pipeline\n",
    "\n",
    "pipe_1st = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('chronology_builder', ChronologyBuilder()),\n",
    "])\n",
    "\n",
    "\n",
    "train = DataSource().X_train\n",
    "train.loc[:,'label'] = DataSource().y_train\n",
    "\n",
    "result = pipe_1st.fit_transform(train)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code length(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceCodeByteCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result['code_size'] = result.html_text \\\n",
    "            .replace(np.nan, '', regex=True) \\\n",
    "            .astype(str) \\\n",
    "            .apply(len)\n",
    "\n",
    "        return result\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('source_code_byte_counter', SourceCodeByteCounter()),\n",
    "])\n",
    "\n",
    "print(type(result))\n",
    "\n",
    "result = pipe.fit_transform(result)\n",
    "\n",
    "result.code_size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### is HTML5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTML5Justifier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result['is_html5'] = result.html_text \\\n",
    "            .replace(np.nan, '', regex=True) \\\n",
    "            .apply(lambda x: int(x.replace('\\n','').replace('\\r','').strip().lower().startswith('<!doctype html>') if x else False))\n",
    "\n",
    "        return result\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('html5_justifier', HTML5Justifier()),\n",
    "])\n",
    "\n",
    "result = pipe.fit_transform(result)\n",
    "\n",
    "result.is_html5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeautifulSoupParserBuilder:\n",
    "\n",
    "    class _BeautifulSoupParser(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self,_lambda_pair):\n",
    "            self._lambda_pair = _lambda_pair\n",
    "\n",
    "        def fit(self,x,y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self,x,y=None):\n",
    "            result = x\n",
    "            soup_handlers = result.html_text \\\n",
    "                    .replace(np.nan, '', regex=True) \\\n",
    "                    .apply(lambda html_doc: BeautifulSoupParserBuilder._safe_create_parser(html_doc))\n",
    "            \n",
    "            for col_name, func in self._lambda_pair.items():\n",
    "                result[col_name] = soup_handlers.apply(func)\n",
    "\n",
    "            return result\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_create_parser(html_doc):\n",
    "        try:\n",
    "            return BeautifulSoup(html_doc, 'html.parser')\n",
    "        except:\n",
    "            return BeautifulSoup('', 'html.parser')\n",
    "        \n",
    "    def __init__(self):\n",
    "        self._lambda_pair = dict()\n",
    "\n",
    "    def add_lambda(self, column_name, lbd):\n",
    "        self._lambda_pair[column_name] = lbd\n",
    "        return self\n",
    "\n",
    "    def build(self):\n",
    "        return BeautifulSoupParserBuilder._BeautifulSoupParser(self._lambda_pair)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Title Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_title_length(soup):\n",
    "    title = soup.title.string if soup.title else ''\n",
    "    if not title:\n",
    "        title = ''\n",
    "    return len(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Types of the JS library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract this feature later when running the association rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### No of JS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_internal_js_lib(soup):\n",
    "    sources=soup.findAll('script',{\"src\":True})\n",
    "    return len([0 for source in sources if not source['src'].startswith('http')])\n",
    "\n",
    "def count_external_js_lib(soup):\n",
    "    sources=soup.findAll('script',{\"src\":True})\n",
    "    return len([0 for source in sources if source['src'].startswith('http')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_charset(soup):\n",
    "    sources=soup.findAll('meta',{\"charset\":True})\n",
    "    if 0 == len(sources):\n",
    "        return ''\n",
    "    return sources[0]['charset'].lower().replace('\\'', '').replace('\"', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### iFrame in Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_iframe(soup):\n",
    "    sources=soup.findAll('iframe')\n",
    "    return int(0 == len(sources))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### No of hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hyperlink(soup):\n",
    "    sources=soup.findAll('a')\n",
    "    return len([1 for source in sources if source.has_attr('href') and source['href'].lower().startswith('http')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Drop the records that does not have html code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyHTMLFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        result = result.dropna(subset=['html_text'])\n",
    "\n",
    "        return result\n",
    "\n",
    "result = EmptyHTMLFilter().fit_transform(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Consolidate the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_parser = BeautifulSoupParserBuilder() \\\n",
    "    .add_lambda('title_length', get_title_length) \\\n",
    "    .add_lambda('internal_js_cnt', count_internal_js_lib) \\\n",
    "    .add_lambda('external_js_cnt', count_external_js_lib) \\\n",
    "    .add_lambda('charset', get_charset) \\\n",
    "    .add_lambda('has_iframe', has_iframe) \\\n",
    "    .add_lambda('hyperlink_cnt', count_hyperlink) \\\n",
    "    .build()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('source_code_byte_counter', SourceCodeByteCounter()),\n",
    "    ('html5_justifier', HTML5Justifier()),\n",
    "    ('empty_html_filter', EmptyHTMLFilter()),\n",
    "    ('html_parser', html_parser),\n",
    "    ('feature_picker', FeaturePicker(['protocol_type',\n",
    "                                      'url_depth',\n",
    "                                      'has_www',\n",
    "                                      'subdomain_level',\n",
    "                                      'param_cnt',\n",
    "                                      'suffix',\n",
    "                                      'timestamp_coef',\n",
    "                                      'is_port_access',\n",
    "                                      'code_size',\n",
    "                                      'title_length',\n",
    "                                      'internal_js_cnt',\n",
    "                                      'external_js_cnt',\n",
    "                                      'charset',\n",
    "                                      'is_html5',\n",
    "                                      'has_iframe',\n",
    "                                      'hyperlink_cnt',\n",
    "                                      'html_text',\n",
    "                                      'label',\n",
    "                                       ])),\n",
    "#     ('logarithm_transformer', LogarithmTransformer(['suffix'])),\n",
    "#     ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "#         encoding_method='frequency',\n",
    "#         variables=['protocol_type'])),\n",
    "#     ('low_variance_remover', LowVarianceRemover(.005)), # Decreased to .005\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "train = DataSource().X_train\n",
    "train.loc[:,'label'] = DataSource().y_train\n",
    "\n",
    "result = pipe.fit_transform(train)\n",
    "\n",
    "result[[\n",
    "    'title_length',\n",
    "    'internal_js_cnt',\n",
    "    'external_js_cnt',\n",
    "    'charset',\n",
    "    'has_iframe',\n",
    "    'hyperlink_cnt']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove tags, Tf-Idf Score of Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tf-Idf Score of Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Second Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[:,'charset'] = result.loc[:,'charset'].apply(lambda x: x.replace('\\'', '').replace('\"', ''))\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.charset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_distribution(result, \"Features Distribution\", height=2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert binary features into numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureValueMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name, mapping):\n",
    "        self._column_name = column_name\n",
    "        self._mapping = mapping\n",
    "\n",
    "    @property\n",
    "    def column_name(self):\n",
    "        return self._column_name\n",
    "\n",
    "    @property\n",
    "    def mapping(self):\n",
    "        return self._mapping\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        result = x\n",
    "        result.loc[:,self._column_name] = result[self._column_name].map(self._mapping)\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('source_code_byte_counter', SourceCodeByteCounter()),\n",
    "    ('html5_justifier', HTML5Justifier()),\n",
    "    ('html_parser', html_parser),\n",
    "    ('chronology_builder', ChronologyBuilder()),\n",
    "    ('binary_feature_converter', FeatureValueMapper('protocol_type', {\n",
    "                                        'http': 1,\n",
    "                                        'https':0,\n",
    "                                        })),\n",
    "    ('feature_picker', FeaturePicker([\n",
    "                                        'protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix',\n",
    "                                        'timestamp_coef',\n",
    "                                        'is_port_access',\n",
    "                                        'code_size',\n",
    "                                        'title_length',\n",
    "                                        'internal_js_cnt',\n",
    "                                        'external_js_cnt',\n",
    "                                        'charset',\n",
    "                                        'is_html5',\n",
    "                                        'has_iframe',\n",
    "                                        'hyperlink_cnt',\n",
    "                                        'label',\n",
    "                                       ])),\n",
    "    ('feature_remover', FeatureRemover([\n",
    "                                        'param_cnt',\n",
    "                                        'is_port_access',\n",
    "                                       ])),\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['charset'])),\n",
    "    ('logarithm_transformer', LogarithmTransformer([\n",
    "                                        'suffix',\n",
    "                                        'title_length',\n",
    "                                        'internal_js_cnt',\n",
    "                                        'external_js_cnt',\n",
    "                                        'hyperlink_cnt',\n",
    "#                                         'protocol_type',\n",
    "#                                         'charset'\n",
    "    ])),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "train = DataSource().X_train\n",
    "train.loc[:,'label'] = DataSource().y_train\n",
    "X_train = pipe.fit_transform(train)\n",
    "y_train = X_train.iloc[:,-1]\n",
    "# X_train = pd.DataFrame(X_train, columns= pipe.steps[-1][1].columns)\n",
    "X_train = X_train.drop('label', axis=1)\n",
    "print(X_train.columns)\n",
    "\n",
    "\n",
    "test = DataSource().X_test\n",
    "test.loc[:,'label'] = DataSource().y_test\n",
    "X_test = pipe.fit_transform(test)\n",
    "y_test = X_test.iloc[:,-1]\n",
    "# X_test = pd.DataFrame(X_test, columns= pipe.steps[-1][1].columns)\n",
    "X_test = X_test.drop('label', axis=1)\n",
    "print(X_test.columns)\n",
    "\n",
    "train.to_csv('train_lr.csv')\n",
    "test.to_csv('test_lr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_copy = X_train.copy()\n",
    "# X_test_copy = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.drop(['code_size', 'is_html5'], axis=1)\n",
    "# X_test = X_test.drop(['code_size', 'is_html5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train_copy\n",
    "# X_test = X_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-4, -1, 50)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                       penalty='elasticnet',\n",
    "                       solver='saga',\n",
    "                       multi_class='ovr',\n",
    "                       warm_start=False,\n",
    "                       n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "\n",
    "# Start to train model\n",
    "engine_lr = AnalysisEngineBuilder() \\\n",
    "    .set_X_train(X_train) \\\n",
    "    .set_y_train(y_train) \\\n",
    "    .set_X_test(X_test) \\\n",
    "    .set_y_test(y_test) \\\n",
    "    .set_param_grid(param_lr) \\\n",
    "    .set_engine(lr) \\\n",
    "    .build()\n",
    "\n",
    "model_lr = engine_lr.analyze()\n",
    "engine_lr.show_performance()\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-4, -1, 50)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                        penalty='elasticnet',\n",
    "                        solver='saga',\n",
    "                        multi_class='ovr',\n",
    "                        warm_start=False,\n",
    "                        n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "loss_accuracy_matrix = calculate_grid_performance(X_train, y_train, X_test, y_test, param_lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizer.plot_performance(data=loss_accuracy_matrix,\n",
    "                    legend_type_name='l1_ratio',\n",
    "                    x_axis_name='C',\n",
    "                    upper_y_label='loss',\n",
    "                    lower_y_label='auc',\n",
    "                    title='Loss& Accuracy - Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizer.plot_importance_trending(X_train, loss_accuracy_matrix, 'Weight change on each feature', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select the hyperparameter and train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "param_lr = {\n",
    "    'l1_ratio': [0.001],\n",
    "    'C': [0.055],\n",
    "    'max_iter': [80],\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                       penalty='elasticnet',\n",
    "                       solver='saga',\n",
    "                       multi_class='ovr',\n",
    "                       warm_start=False,\n",
    "                       n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "\n",
    "# Start to train model\n",
    "engine_lr = AnalysisEngineBuilder() \\\n",
    "    .set_X_train(X_train) \\\n",
    "    .set_y_train(y_train) \\\n",
    "    .set_X_test(X_test) \\\n",
    "    .set_y_test(y_test) \\\n",
    "    .set_param_grid(param_lr) \\\n",
    "    .set_engine(lr) \\\n",
    "    .build()\n",
    "\n",
    "model_lr = engine_lr.analyze()\n",
    "engine_lr.show_performance()\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "engine_lr.fpr\n",
    "\n",
    "Visualizer.group_plot_roc_curve('ROC Curve of Logistic Regression', [\n",
    "    (engine_lr.fpr, engine_lr.tpr, 'Logistic Regression')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizer.plot_feature_importance(\n",
    "    model_lr.best_estimator_.coef_[0], X_train.columns, \n",
    "    \"Coefficients in the Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Third Round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the features from MAG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singleton\n",
    "class NanToZeroConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self._columns = columns\n",
    "\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self,x,y=None):\n",
    "        result = x\n",
    "        self._columns = [col_name for col_name in self._columns if col_name in result.columns]\n",
    "        for col_name in self._columns:\n",
    "            result.loc[:, col_name] = result[col_name].fillna(0)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('url_length_counter', URLLengthCounter()),\n",
    "    ('url_depth_counter', URLDepthCounter()),\n",
    "    ('has_www_converter', HasWWWConverter()),\n",
    "    ('subdomain_level_counter', SubdomainLevelCounter()),\n",
    "    ('request_parameter_counter', RequestParameterCounter()),\n",
    "    ('domain_suffix_builder', DomainSuffixBuilder()),\n",
    "    ('incorrect_domain_url_cleaner', IncorrectDomainUrlCleaner()),\n",
    "    ('column_renamer', ColumnRenamer({'scheme': 'protocol_type'})),\n",
    "    ('source_code_byte_counter', SourceCodeByteCounter()),\n",
    "    ('html5_justifier', HTML5Justifier()),\n",
    "    ('html_parser', html_parser),\n",
    "    ('chronology_builder', ChronologyBuilder()),\n",
    "    ('binary_feature_converter', FeatureValueMapper('protocol_type', {\n",
    "                                        'http': 1,\n",
    "                                        'https':0,\n",
    "                                        })),\n",
    "    ('nan_to_Zero_converter', NanToZeroConverter(\n",
    "        'total_num_of_paper_citing',\n",
    "        'total_num_of_author_citing',\n",
    "        'total_num_of_affiliation_citing',\n",
    "        'total_num_of_journal_citing',\n",
    "        'total_num_of_author_self_citation',\n",
    "        'total_num_of_affiliation_self_citation',\n",
    "        'total_num_of_journal_self_citation',\n",
    "        'avg_year',\n",
    "        'min_year',\n",
    "        'max_year',\n",
    "        'median',\n",
    "        'num_of_author',\n",
    "        'num_of_author_citing',\n",
    "        'num_of_affiliation_citing',\n",
    "        'num_of_journal_citing',\n",
    "        'avg_hindex',\n",
    "        'first_author_hindex',\n",
    "        'last_author_hindex',\n",
    "        'avg_mid_author_hindex',\n",
    "        'paper_unique_affiliation'\n",
    "    )),\n",
    "    \n",
    "    ('feature_picker', FeaturePicker([\n",
    "                                        'protocol_type',\n",
    "                                        'url_depth',\n",
    "                                        'has_www',\n",
    "                                        'subdomain_level',\n",
    "                                        'param_cnt',\n",
    "                                        'suffix',\n",
    "                                        'timestamp_coef',\n",
    "                                        'is_port_access',\n",
    "                                        'code_size',\n",
    "                                        'title_length',\n",
    "                                        'internal_js_cnt',\n",
    "                                        'external_js_cnt',\n",
    "                                        'charset',\n",
    "                                        'is_html5',\n",
    "                                        'has_iframe',\n",
    "                                        'hyperlink_cnt',\n",
    "\n",
    "                                        'total_num_of_paper_citing',\n",
    "                                        'total_num_of_author_citing',\n",
    "                                        'total_num_of_affiliation_citing',\n",
    "                                        'total_num_of_journal_citing',\n",
    "                                        'total_num_of_author_self_citation',\n",
    "                                        'total_num_of_affiliation_self_citation',\n",
    "                                        'total_num_of_journal_self_citation',\n",
    "                                        'avg_year',\n",
    "                                        'min_year',\n",
    "                                        'max_year',\n",
    "                                        'median',\n",
    "                                        'num_of_author',\n",
    "                                        'num_of_author_citing',\n",
    "                                        'num_of_affiliation_citing',\n",
    "                                        'num_of_journal_citing',\n",
    "                                        'avg_hindex',\n",
    "                                        'first_author_hindex',\n",
    "                                        'last_author_hindex',\n",
    "                                        'avg_mid_author_hindex',\n",
    "                                        'paper_unique_affiliation',\n",
    "\n",
    "                                        'label',\n",
    "                                       ])),\n",
    "    ('feature_remover', FeatureRemover([\n",
    "                                        'param_cnt',\n",
    "                                        'is_port_access',\n",
    "                                       ])),\n",
    "    ('frequency_indexer', categorical_encoders.CountFrequencyCategoricalEncoder(\n",
    "        encoding_method='frequency',\n",
    "        variables=['charset'])),\n",
    "    ('logarithm_transformer', LogarithmTransformer([\n",
    "                                        'suffix',\n",
    "                                        'title_length',\n",
    "                                        'internal_js_cnt',\n",
    "                                        'external_js_cnt',\n",
    "                                        'hyperlink_cnt',\n",
    "    ])),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataSource().X_train\n",
    "train.loc[:,'label'] = DataSource().y_train\n",
    "X_train = pipe.fit_transform(train)\n",
    "y_train = X_train.iloc[:,-1]\n",
    "# X_train = pd.DataFrame(X_train, columns= pipe.steps[-1][1].columns)\n",
    "X_train = X_train.drop('label', axis=1)\n",
    "print(X_train.columns)\n",
    "\n",
    "\n",
    "test = DataSource().X_test\n",
    "test.loc[:,'label'] = DataSource().y_test\n",
    "X_test = pipe.fit_transform(test)\n",
    "y_test = X_test.iloc[:,-1]\n",
    "# X_test = pd.DataFrame(X_test, columns= pipe.steps[-1][1].columns)\n",
    "X_test = X_test.drop('label', axis=1)\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-4, -1, 50)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                       penalty='elasticnet',\n",
    "                       solver='saga',\n",
    "                       multi_class='ovr',\n",
    "                       warm_start=False,\n",
    "                       n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "\n",
    "# Start to train model\n",
    "engine_lr = AnalysisEngineBuilder() \\\n",
    "    .set_X_train(X_train) \\\n",
    "    .set_y_train(y_train) \\\n",
    "    .set_X_test(X_test) \\\n",
    "    .set_y_test(y_test) \\\n",
    "    .set_param_grid(param_lr) \\\n",
    "    .set_engine(lr) \\\n",
    "    .build()\n",
    "\n",
    "model_lr = engine_lr.analyze()\n",
    "engine_lr.show_performance()\n",
    "\n",
    "t = str(datetime.timedelta(seconds=time.time() - start_time)).split(':')\n",
    "print(\"--- %s minutes, %.2f seconds ---\" % (t[1], float(t[2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "engine_lr.fpr\n",
    "\n",
    "Visualizer.group_plot_roc_curve('ROC Curve of Logistic Regression', [\n",
    "    (engine_lr.fpr, engine_lr.tpr, 'Logistic Regression')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lr = {\n",
    "    'l1_ratio': [0, *np.logspace(-3, 0, 5)],\n",
    "    'C': sorted(np.logspace(-4, -1, 50)),\n",
    "    'max_iter': np.arange(10,80,40),\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=seed,\n",
    "                        penalty='elasticnet',\n",
    "                        solver='saga',\n",
    "                        multi_class='ovr',\n",
    "                        warm_start=False,\n",
    "                        n_jobs=allocated_cpu,\n",
    ")\n",
    "\n",
    "loss_accuracy_matrix = calculate_grid_performance(X_train, y_train, X_test, y_test, param_lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizer.plot_performance(data=loss_accuracy_matrix,\n",
    "                    legend_type_name='l1_ratio',\n",
    "                    x_axis_name='C',\n",
    "                    upper_y_label='loss',\n",
    "                    lower_y_label='auc',\n",
    "                    title='Loss& Accuracy - Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizer.plot_importance_trending(X_train, loss_accuracy_matrix, 'Weight change on each feature', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
