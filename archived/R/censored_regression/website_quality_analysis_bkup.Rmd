---
title: "Website Quality Analysis"
author: "Jian Jian"
date: "6/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Environment Setup

Presetup

```{r tobit}
require(ggplot2)
require(GGally)
require(VGAM)

# Give the input file name to the function.

raw_data <- read.csv("~/Desktop/wq.csv")
raw_data <- raw_data[which(colnames(raw_data) != "label")]
#df <- read.csv("~/Desktop/tobit.csv")
summary(raw_data)
```
Split the data

```{r}
require(tidyverse)
require(caret)
split=0.70
train_idx <- createDataPartition(raw_data$last_available_timestamp, p=split, list=FALSE)
train_data <- raw_data %>%
  slice(train_idx)
test_data <- raw_data %>%
  slice(-train_idx)

X_train <- train_data %>% 
  select(-last_available_timestamp)
X_test <- test_data %>% 
  select(-last_available_timestamp)

y_train <- train_data %>% 
  select(last_available_timestamp)
y_test <- test_data %>% 
  select(last_available_timestamp)
```


K-Fold

```{r}
folds <- createFolds(X_train$protocol_type, k = 10)
```

## Training

Tobit Regression uses the log likeihood ratio to estimate the case when actual y value exceeded the value presented in the dataset.
```{r training, echo=FALSE}
require("remotes")
remotes::install_github("USGS-R/smwrQW")
```

```{r training, echo=FALSE}
label_name <- "last_available_timestamp"
c_name <- colnames(raw_data)
c_name <- c_name[which(c_name != label_name)]
# fn <- as.formula(paste(label_name, paste(c_name, collapse = " + "), sep = " ~ "))
fn <- last_available_timestamp ~ protocol_type + has_www + has_iframe + 
    int + org + gov + in. + eu + cn + url_depth + subdomain_level + 
    param_cnt + title_length + internal_js_cnt + external_js_cnt + 
    charset + hyperlink_cnt + total_num_of_paper_citing + total_num_of_author_citing + 
    total_num_of_affiliation_citing + total_num_of_journal_citing + 
    total_num_of_author_self_citation + total_num_of_affiliation_self_citation + 
    total_num_of_journal_self_citation + avg_year + min_year + 
    max_year + median + num_of_author + num_of_author_citing + 
    num_of_affiliation_citing + num_of_journal_citing + avg_hindex + 
    first_author_hindex + avg_mid_author_hindex + 
    paper_unique_affiliation + first_appear

ul <- max(raw_data[label_name])
best_model <- NULL
best_r2 <- 0

for (i in length(folds)) {
  
  model <- vglm(fn, tobit(Upper = max(ul)), data = train_data[folds[[i]],])
  perf <- list()
  perf$y <- train_data[folds[[i]],]$last_available_timestamp
  perf$yhat <- fitted(model)[,1]
  perf$r2 <- with(perf, cor(yhat, y)) ^ 2
  if (perf$r2 > best_r2) {
    best_r2 <- perf$r2
    best_model <- model
  }
}

summary(model)
```

Calculate the p value
```{r summary, echo=FALSE}
predict(best_model, newdata = test_data)
fitted(best_model)[,1]
ctable <- coef(summary(model))
pvals <- 2 * pt(abs(ctable[, "z value"]), test_data.residual(model), lower.tail = FALSE)
result <- cbind(ctable, pvals)
```
Display
```{r}
result
```
R square
```{r }
perf$yhat <- fitted(model)[,1]
perf$r2 <- with(df, cor(yhat, last_available_timestamp)) ^ 2
(r <- with(df, cor(yhat, last_available_timestamp)))
r^2
```